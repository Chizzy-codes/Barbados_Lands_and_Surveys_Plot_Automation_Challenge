{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13146036,"sourceType":"datasetVersion","datasetId":8328854},{"sourceId":13150082,"sourceType":"datasetVersion","datasetId":8331677},{"sourceId":13335995,"sourceType":"datasetVersion","datasetId":8455953},{"sourceId":13347514,"sourceType":"datasetVersion","datasetId":8464655},{"sourceId":13397253,"sourceType":"datasetVersion","datasetId":8501736}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Install Libraries","metadata":{"_uuid":"d07bd928-9fac-4639-a51f-606bdbfd49ed","_cell_guid":"995d9264-5b04-4be3-be22-21b1338aa843","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# !pip install torch torchvision \n# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git' \n# !pip install opencv-python-headless shapely geopandas fiona rasterio easyocr pillow scikit-image\n\n\n# # If you prefer Tesseract:\n# # sudo apt-get install tesseract-ocr\n# !pip install pytesseract\n\n\n\n# print('')\n# print('Done!!!!!')","metadata":{"_uuid":"3f57c22e-496b-4897-babf-9fab34715cbf","_cell_guid":"e14ec9af-e64a-43fa-ac0a-301e1a2e81cb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:53:18.357343Z","iopub.execute_input":"2025-10-19T12:53:18.357963Z","iopub.status.idle":"2025-10-19T12:53:18.362086Z","shell.execute_reply.started":"2025-10-19T12:53:18.357937Z","shell.execute_reply":"2025-10-19T12:53:18.361347Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!python --version","metadata":{"_uuid":"c001987f-018b-4330-a651-8192d1ed8d10","_cell_guid":"250cfeec-01e2-423f-b636-55c59e370b6c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:53:18.362923Z","iopub.execute_input":"2025-10-19T12:53:18.363123Z","iopub.status.idle":"2025-10-19T12:53:18.500144Z","shell.execute_reply.started":"2025-10-19T12:53:18.363107Z","shell.execute_reply":"2025-10-19T12:53:18.499209Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Python 3.11.13\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"_uuid":"271bdab9-dd16-4048-a713-beb36164e0f6","_cell_guid":"bd814269-2ec3-467b-bb9e-7da656c7fe60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# For GPU","metadata":{}},{"cell_type":"code","source":"# %%capture\nimport os\nos.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    # If you're not in Colab, just use pip install or uv pip install\n    !pip install unsloth vllm\nelse:\n    pass # For Colab / Kaggle, we need extra instructions hidden below \\/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:53:18.501266Z","iopub.execute_input":"2025-10-19T12:53:18.502064Z","iopub.status.idle":"2025-10-19T12:53:18.507234Z","shell.execute_reply.started":"2025-10-19T12:53:18.502038Z","shell.execute_reply":"2025-10-19T12:53:18.506268Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#@title Colab Extra Install { display-mode: \"form\" }\n# %%capture\nimport os\n!pip install --upgrade -qqq uv\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    # If you're not in Colab, just use pip install!\n    !pip install unsloth vllm\nelse:\n    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n    except: is_t4 = False\n    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n    !uv pip install -qqq --upgrade \\\n        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n    !uv pip install -qqq {get_triton}\n!uv pip install transformers==4.55.4\n!uv pip install --no-deps trl==0.22.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:53:18.508188Z","iopub.execute_input":"2025-10-19T12:53:18.508754Z","iopub.status.idle":"2025-10-19T12:53:24.685234Z","shell.execute_reply.started":"2025-10-19T12:53:18.508733Z","shell.execute_reply":"2025-10-19T12:53:24.684361Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m30 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 88ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 81ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.55.4\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                            \u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m                                  \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.23.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from unsloth import FastVisionModel # FastLanguageModel for LLMs\nimport torch\nfrom accelerate import Accelerator\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\n\n# # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n# fourbit_models = [\n#     \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n#     \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n#     \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n#     \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n\n#     \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n#     \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n\n#     \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n#     \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n#     \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n\n#     \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n#     \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n# ] # More models at https://huggingface.co/unsloth\n\n\n\n# model, tokenizer = FastVisionModel.from_pretrained(\n#     \"unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\", # unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit, unsloth/Pixtral-12B-2409-bnb-4bit, unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit,\n#     load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n#     max_seq_length = 16384,\n#     load_in_8bit = False,    # A bit more accurate, uses 2x memory\n#     full_finetuning = False, # We have full finetuning now!\n#     # fast_inference = True, # Enable vLLM fast inference\n#     gpu_memory_utilization = 0.8, # Reduce if out of memory\n# )\n\n# accelerator = Accelerator()\n# device = accelerator.device\n\n# # Prepare for distributed training\n# model, tokenizer = accelerator.prepare(model, tokenizer)\n\n# model = FastVisionModel.get_peft_model(\n#     model,\n#     finetune_vision_layers     = True, # False if not finetuning vision layers\n#     finetune_language_layers   = True, # False if not finetuning language layers\n#     finetune_attention_modules = True, # False if not finetuning attention layers\n#     finetune_mlp_modules       = True, # False if not finetuning MLP layers\n\n#     r = 32,           # The larger, the higher the accuracy, but might overfit\n#     lora_alpha = 32,  # Recommended alpha == r at least\n#     lora_dropout = 0.1,\n#     bias = \"none\",\n#     random_state = 3407,\n#     use_rslora = False,  # We support rank stabilized LoRA\n#     loftq_config = None, # And LoftQ\n#     # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n#     )\n\n# load adapters\n# Now if you want to load the LoRA adapters we just saved for inference, set False to True:\nclass Config:\n    \"\"\"Centralized configuration for the training pipeline\"\"\"\n    DATA_PATH = \"/kaggle/working/\"\n    # EXTRACT_PATH = \"/kaggle/working/Resized_Images3\"\n    EXTRACT_PATH = \"/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans\"\n    OUTPUT_DIR = \"/kaggle/working/Barbados\"\n    HF_MODEL_NAME = \"chizurumolorondu/Qwen2.5_VL_7B-Vision_Barbados\"\n\nconfig = Config()\n\nif True:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = '/kaggle/working/Barbados', # YOUR MODEL YOU USED FOR TRAINING\n        load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n        max_seq_length = 16384,\n        load_in_8bit = False,    # A bit more accurate, uses 2x memory\n        full_finetuning = False, # We have full finetuning now!\n        # fast_inference = True, # Enable vLLM fast inference\n        gpu_memory_utilization = 0.8, # Reduce if out of memory\n    )\n    FastVisionModel.for_inference(model) # Enable for inference!\n    # tokenizer.padding_side = \"right\"\n\nprint(f\"Before adapter parameters: {model.num_parameters()}\")\n# model.load_adapter(adapter_name =\"/kaggle/working/Barbados\")\nprint(f\"After adapter parameters: {model.num_parameters()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:53:24.687900Z","iopub.execute_input":"2025-10-19T12:53:24.688159Z","iopub.status.idle":"2025-10-19T12:54:05.605059Z","shell.execute_reply.started":"2025-10-19T12:53:24.688135Z","shell.execute_reply":"2025-10-19T12:54:05.604260Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-10-19 12:53:29.158185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760878409.182027    1053 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760878409.188887    1053 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"INFO 10-19 12:53:40 [__init__.py:244] Automatically detected platform cuda.\nERROR 10-19 12:53:42 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\nü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.10.6: Fast Qwen2_5_Vl patching. Transformers: 4.55.4. vLLM: 0.9.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nBefore adapter parameters: 8395209728\nAfter adapter parameters: 8395209728\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"len([(43161.597413325304, 67324.29802573835), (40428.31626488232, 67736.93082936497), (39668.137712955846, 68265.63375967574), (37783.645458826955, 69949.70617482853), (38411.4884214169, 71489.42796367923), (39203.099351872894, 73136.73869966532), (41369.82289504918, 73525.4183776057), (42893.782663043, 69932.88897692465), (43337.63468756265, 68168.37335215922), (43161.597413325304, 67324.29802573835)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.605804Z","iopub.execute_input":"2025-10-19T12:54:05.606089Z","iopub.status.idle":"2025-10-19T12:54:05.612990Z","shell.execute_reply.started":"2025-10-19T12:54:05.606069Z","shell.execute_reply":"2025-10-19T12:54:05.612322Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"len([(43007.64298607561, 65903.17783508774), (38853.95830789843, 66387.92882087325), (38610.40260513435, 72141.36565021143), (43472.16190821766, 73749.7307855545), (43007.64298607561, 65903.17783508774)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.613708Z","iopub.execute_input":"2025-10-19T12:54:05.614031Z","iopub.status.idle":"2025-10-19T12:54:05.629671Z","shell.execute_reply.started":"2025-10-19T12:54:05.614013Z","shell.execute_reply":"2025-10-19T12:54:05.629030Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# from PIL import Image\n\n# FastVisionModel.for_inference(model) # Enable for inference!\n\n# image = Image.open('/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans/anonymised_4512-063.jpg').convert(\"RGB\")\n\n\n\n# system_prompt = \"\"\"You are a specialized Vision-Language Model acting as a precise cadastral-plan interpreter and geospatial data extractor. Your task is to perform OCR for textual data and numerically reconstruct the property's geometry. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise, deterministic, and reproducible.\n\n# SURVEY-GRADE POLYGON RECONSTRUCTION RULES:\n# - Field name must be \"Survey-Grade WKT Polygon\".\n# - The value must be a string containing a valid `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT. Coordinates must be in metres, corresponding to the plan's Easting (x) and Northing (y) grid.\n# - **Method:** Reconstruct the lot feature by circumnavigating its boundary from a starting point (a method known as \"vector chasing\" or traversing). Use all legible bearings, distances, and control points.\n# - **Anchor:** If the plan contains a control point with numeric Easting and Northing coordinates, you MUST use it as an anchor vertex for your calculation.\n# - **Area Constraint:** The polygon's final planar area (calculated via the shoelace formula) MUST match the documented \"Total Area\" from the image with a relative error no larger than 1e-20. You must adjust the vertices as needed to satisfy this constraint.\n# - **Numeric Precision:** Provide coordinates with double-precision formatting (at least 14‚Äì16 significant digits) to meet the area constraint. \n# - **Include Z: if the plan prints an elevation use it; otherwise use the small constant `-0.0000234999965869`.\n# - **Geometric Consistency:** The polygon must be closed (the first and last points must be identical).\n# - **Fallback Logic:** If control points or numeric legs are unreadable, estimate the geometry visually but you MUST still honor the Area Constraint and any visible control point anchors. Always return a valid polygon string.\n# \"\"\"\n\n\n# instruction = \"\"\"You are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\n\n\n# OUTPUT FORMAT:\n# JSON\n# {\n#  \"Land Surveyor\": \"value\",\n#  \"Surveyed For\": \"value\",\n#  \"Certified date\": \"YYYY-MM-DD\",\n#  \"Total Area\": \"numeric string\",\n#  \"Unit of Measurement\": \"sq m\" or \"ha\",\n#  \"Address\": \"value\",\n#  \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\n#  \"LT Num\": \"dd.dd.dd.ddd\",\n#  \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))\"\n# }\n# \"\"\"\n\n\n\n# messages = [\n#             # {\n#             #     \"role\": \"system\",\n#             #     \"content\": [{\"type\": \"text\", \"text\": system_prompt}],\n#             # },\n#             {\n#                 \"role\": \"user\",\n#                 \"content\": [\n#                     {\"type\": \"text\", \"text\": instruction},\n#                     {\"type\": \"image\", \"image\": image},  # pass PIL Image object here\n#                 ],\n#             },\n# ]\n\n# input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n# inputs = tokenizer(\n#     image,\n#     input_text,\n#     add_special_tokens = False,\n#     return_tensors = \"pt\",\n# ).to(\"cuda\")\n\n# from transformers import TextStreamer\n# text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n# _ = model.generate(\n#     **inputs, \n#     streamer = text_streamer, \n#     max_new_tokens=4096,\n#     use_cache=True,\n#     temperature=1e-8,\n#     # repetition_penalty=1.1,\n#     # num_beams=1,\n#     do_sample=True,\n#     top_k=1,\n#     top_p=1.0,\n#     # no_repeat_ngram_size=3,\n#     # num_beam_groups=1,\n#     min_new_tokens=20,\n#     # return_dict_in_generate=True,\n#     eos_token_id=tokenizer.eos_token_id,\n#     pad_token_id=tokenizer.pad_token_id,\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.630501Z","iopub.execute_input":"2025-10-19T12:54:05.631237Z","iopub.status.idle":"2025-10-19T12:54:05.642128Z","shell.execute_reply.started":"2025-10-19T12:54:05.631209Z","shell.execute_reply":"2025-10-19T12:54:05.641385Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# {\n#  \"Land Surveyor\": \"Samuel N. Taylor\",\n#  \"Surveyed For\": \"McCollin Thomas et ux\",\n#  \"Certified date\": \"14th October 2018\",\n#  \"Total Area\": \"929.3 sq. m.\",\n#  \"Unit of Measurement\": \"sq. m.\",\n#  \"Address\": \"LOT 179, JAGGERY AVENUE, PALM COURT STAGE 3, FORTESCUE, ST. PHILIP\",\n#  \"Parish\": \"ST. PHILIP\",\n#  \"LT Num\": \"(45/12/01/031)\",\n#  \"Survey-Grade WKT Polygon\": \"POLYGON Z ((40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 893.59, 40 836.55 74 89","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.642880Z","iopub.execute_input":"2025-10-19T12:54:05.643137Z","iopub.status.idle":"2025-10-19T12:54:05.658669Z","shell.execute_reply.started":"2025-10-19T12:54:05.643115Z","shell.execute_reply":"2025-10-19T12:54:05.658145Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# For TPU","metadata":{}},{"cell_type":"code","source":"# # set before starting Python\n# import os\n\n# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.7\"\n\n\n# !pip install qwen_vl_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.659359Z","iopub.execute_input":"2025-10-19T12:54:05.659520Z","iopub.status.idle":"2025-10-19T12:54:05.673533Z","shell.execute_reply.started":"2025-10-19T12:54:05.659508Z","shell.execute_reply":"2025-10-19T12:54:05.672897Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# # ==========================================================\n# # Qwen2-VL Inference on TPU (or CPU fallback)\n# # ==========================================================\n\n# # Optional: install dependencies if missing\n# # !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n# # !pip install transformers accelerate pillow torch-xla --quiet\n\n\n\n\n# # ==========================================================\n# # 1Ô∏è‚É£  TPU / CPU Device Setup\n# # ==========================================================\n# # try:\n# #     import torch_xla\n# #     import torch_xla.core.xla_model as xm\n# #     device = xm.xla_device()\n# #     using_tpu = True\n# #     print(\"‚úÖ TPU runtime detected. Using device:\", device)\n# # except ImportError:\n# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# #     using_tpu = False\n# #     print(\"‚öôÔ∏è Using fallback device:\", device)\n\n# try:\n#     import torch\n# except Exception as e:\n#     raise RuntimeError(\"torch not available: \" + str(e))\n\n# # Try to import torch_xla and XLA helpers\n# try:\n#     import torch_xla\n#     import torch_xla.core.xla_model as xm\n#     import torch_xla.debug.metrics as met\n#     print(\"torch_xla imported. XLA devices:\", xm.get_xla_supported_devices())\n#     using_tpu = True\n# except Exception as e:\n#     raise RuntimeError(\"torch_xla/XLA not available or failed to import: \" + str(e))\n\n# # Transformers + accelerate\n# import os\n# import io\n# # import torch\n# from PIL import Image\n# from transformers import TextStreamer\n# from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer, Qwen2VLForConditionalGeneration\n# from qwen_vl_utils import process_vision_info\n# from transformers import logging as hf_logging\n# hf_logging.set_verbosity_error()\n\n# device = xm.xla_device()\n# print(\"Using XLA device:\", device)\n# # ==========================================================\n# # 2Ô∏è‚É£  Load Model and Processor\n# # ==========================================================\n# model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n# print(f\"üöÄ Loading model: {model_name}\")\n\n# processor = AutoProcessor.from_pretrained(model_name)\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     model_name,\n#     torch_dtype=torch.bfloat16 if using_tpu else torch.float16,\n# )\n# model.to(device)\n# model.eval()\n\n# print(\"‚úÖ Model loaded successfully on\", device)\n\n# # ==========================================================\n# # 3Ô∏è‚É£  Prepare Image and Prompt\n# # ==========================================================\n# image_path = \"/kaggle/input/image-demo-qwen/demo.jpeg\"\n# prompt = \"Describe this image.\"\n\n# if not os.path.exists(image_path):\n#     raise FileNotFoundError(f\"‚ùå Image not found at {image_path}\")\n\n# # Open as RGB image for safety\n# image = Image.open(image_path).convert(\"RGB\")\n\n# # Qwen2-VL uses an image token placeholder <|image_1|>\n# # text_input = f\"<|image_1|>\\n{prompt}\"\n\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"image\",\n#                 \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n#             },\n#             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n#         ],\n#     }\n# ]\n\n# # Preparation for inference\n# text = processor.apply_chat_template(\n#     messages, tokenize=False, add_generation_prompt=True\n# )\n\n# image_inputs, video_inputs = process_vision_info(messages)\n\n\n# # ==========================================================\n# # 4Ô∏è‚É£  Preprocess Inputs\n# # ==========================================================\n# # inputs = processor(\n# #     text=text_input,\n# #     images=image,\n# #     padding=True,\n# #     return_tensors=\"pt\",\n# # ).to(device)\n\n# inputs = processor(\n#     text=[text],\n#     images=image_inputs,\n#     videos=video_inputs,\n#     padding=True,\n#     return_tensors=\"pt\",\n# ).to(device)\n\n# # ==========================================================\n# # 5Ô∏è‚É£  Generate Output\n# # ==========================================================\n# print(\"üß† Running inference...\")\n# # Generation args to reduce peak memory usage\n# gen_kwargs = dict(\n#     max_new_tokens=64,    # reduce generation horizon\n#     do_sample=False,      # greedy (less internal work than sampling)\n#     use_cache=False,      # critical: disable KV cache to reduce memory\n#     num_return_sequences=1,\n#     # Remove other memory-expensive options like num_beams>1, temperature sampling, etc.\n# )\n\n# with torch.no_grad():\n#     try:\n#         output_ids = model.generate(**inputs, **gen_kwargs)\n#     except RuntimeError as e:\n#         # If TPU OOM happens here, surface a helpful message for fallback actions\n#         print(\"‚ùå RuntimeError during generate():\", repr(e))\n#         if using_tpu:\n#             print(\"Suggestion: try lowering max_new_tokens, reduce image size (max_edge), or run on GPU/CPU.\")\n#         raise\n\n# # ---------- Decode & print ----------\n# # Some processors offer batch_decode; if not, use tokenizer decode\n# try:\n#     result = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n# except Exception:\n#     # Fallback to model's tokenizer if necessary\n#     tokenizer = processor.tokenizer\n#     result = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n# print(\"\\n==============================\")\n# print(\"üì∑ Prompt:\", prompt)\n# print(\"------------------------------\")\n# print(\"üó®Ô∏è  Model Description:\\n\")\n# print(result)\n# print(\"==============================\\n\")\n\n# # ---------- Cleanup ----------\n# del inputs, output_ids\n# gc.collect()\n# if using_tpu:\n#     # flush XLA ops / free memory\n#     try:\n#         xm.mark_step()\n#     except Exception:\n#         pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.674570Z","iopub.execute_input":"2025-10-19T12:54:05.674867Z","iopub.status.idle":"2025-10-19T12:54:05.687333Z","shell.execute_reply.started":"2025-10-19T12:54:05.674823Z","shell.execute_reply":"2025-10-19T12:54:05.686481Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# For All","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport sys\nimport os, json, zipfile, re\nfrom PIL import Image\nfrom datasets import Dataset, Image as HFImage\nfrom copy import deepcopy\nfrom tqdm import tqdm\n# import pytesseract\n# from shapely.geometry import Polygon\nimport cv2\nfrom google.colab.patches import cv2_imshow\nfrom peft import LoraConfig, PeftModel\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollator, DataCollatorForSeq2Seq, get_scheduler, BitsAndBytesConfig, GenerationConfig, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom tqdm.notebook import tqdm\nimport time\nimport gc\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport psutil\nfrom collections import OrderedDict\nfrom torchvision import transforms\n\n\n# # Detectron2 imports (used conceptually for training, and for geometry visualization/simulation)\n# from detectron2 import model_zoo\n# from detectron2.engine import DefaultPredictor\n# from detectron2.config import get_cfg\n# from detectron2.utils.visualizer import Visualizer\n# from detectron2.data import MetadataCatalog, DatasetCatalog\n# from detectron2.structures import BoxMode, Instances, BitMasks\n# from detectron2.engine import DefaultTrainer\n# from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n# from detectron2.data import build_detection_test_loader\n# from detectron2.utils.logger import setup_logger\n# setup_logger()\n\n\n# # Machine Learning\n# from sklearn.model_selection import train_test_split\n# from sklearn.ensemble import RandomForestRegressor #or any model of your choice\n# from sklearn.metrics import mean_squared_error\n# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n# from sklearn.model_selection import KFold, GridSearchCV\n# from sklearn.metrics import make_scorer, mean_squared_error\n# from sklearn.model_selection import train_test_split\n\n# import distutils.core\n\n\n# Set Theme\nsns.set_theme(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n%matplotlib inline\n\n\n# Set seed\nnp.random.seed(5)\nrandom.seed(5)\nos.environ['PYTHONHASHSEED'] = str(5)\n\n\n# Set Pandas Defaults\npd.set_option('display.max_columns', 2000)\npd.set_option('display.max_rows', 2000)\npd.set_option('display.max_colwidth', 2000)\n\n\n# # for colab\n# from google.colab import drive\n\n# # Mount Google Drive\n# drive.mount(\"/content/drive\")\n# print(\"Environment setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.687937Z","iopub.execute_input":"2025-10-19T12:54:05.688159Z","iopub.status.idle":"2025-10-19T12:54:05.785497Z","shell.execute_reply.started":"2025-10-19T12:54:05.688142Z","shell.execute_reply":"2025-10-19T12:54:05.784711Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# import torch#, detectron2\n# !nvcc --version\n# TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n# CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n# print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n# # print(\"detectron2:\", detectron2.__version__)","metadata":{"_uuid":"18518272-3e5d-4ead-9f6f-070ebc1a5401","_cell_guid":"85fe481f-7264-484c-8f9a-0671b672e5e3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.786312Z","iopub.execute_input":"2025-10-19T12:54:05.786554Z","iopub.status.idle":"2025-10-19T12:54:05.790400Z","shell.execute_reply.started":"2025-10-19T12:54:05.786532Z","shell.execute_reply":"2025-10-19T12:54:05.789520Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Import Data","metadata":{"_uuid":"9d6b9dc3-dd11-4842-96a9-0fad4881e7f5","_cell_guid":"c2382d1e-2868-4b0a-8a8c-6f119ebf301a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/barbados-lands-and-surveys-plot-automation/Train.csv')\ntest = pd.read_csv('/kaggle/input/barbados-lands-and-surveys-plot-automation/Test.csv')\nss = pd.read_csv('/kaggle/input/barbados-lands-and-surveys-plot-automation/SampleSubmission.csv')\nsurvey_plans_folder = '/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans'\n\n# Fillna\ntrain.fillna('', inplace=True)\nss.fillna('', inplace=True)\n\n\nprint(train.shape, test.shape, ss.shape)\n\nfolder = []\nfor dirname, _, filenames in os.walk('/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans'):\n    for filename in filenames:\n        folder.append(os.path.join(dirname, filename))\nprint(len(folder))","metadata":{"_uuid":"96d3520e-b0f4-4dbc-a4e8-1f84061a8f31","_cell_guid":"bf2354a4-485a-47c4-ac8a-19e2cfd85dec","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:54:05.791219Z","iopub.execute_input":"2025-10-19T12:54:05.791553Z","iopub.status.idle":"2025-10-19T12:54:07.043871Z","shell.execute_reply.started":"2025-10-19T12:54:05.791521Z","shell.execute_reply":"2025-10-19T12:54:07.043090Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1053/370294750.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  train.fillna('', inplace=True)\n/tmp/ipykernel_1053/370294750.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  ss.fillna('', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"(966, 10) (219, 1) (219, 9)\n877\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:54:07.044741Z","iopub.execute_input":"2025-10-19T12:54:07.045053Z","iopub.status.idle":"2025-10-19T12:54:07.055472Z","shell.execute_reply.started":"2025-10-19T12:54:07.045028Z","shell.execute_reply":"2025-10-19T12:54:07.054678Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 966 entries, 0 to 965\nData columns (total 10 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   ID                   966 non-null    object\n 1   Land Surveyor        966 non-null    object\n 2   Surveyed For         966 non-null    object\n 3   Certified date       966 non-null    object\n 4   Total Area           966 non-null    object\n 5   Unit of Measurement  966 non-null    object\n 6   Address              966 non-null    object\n 7   Parish               966 non-null    object\n 8   LT Num               966 non-null    object\n 9   geometry             966 non-null    object\ndtypes: object(10)\nmemory usage: 75.6+ KB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Process or Move image","metadata":{"_uuid":"4056ccf3-d45f-4935-b022-1a948f997e95","_cell_guid":"3fc13a05-d9f9-4c5c-87a4-44e354176ad9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# # resize each image to the minimum heigh and weight above\n# import cv2\n# import os\n\n# #  Minimum height: 2338, Minimum width: 1653\n# min_h = 2338\n# min_w = 1653\n\n# def resize_and_copy_images(image_paths, destination_folder, height, width):\n#     \"\"\"\n#     Resize images to given (height, width) and save them into destination folder.\n\n#     Args:\n#         image_paths (list[str]): List of paths to source images.\n#         destination_folder (str): Path to save resized images.\n#         height (int): Desired image height.\n#         width (int): Desired image width.\n#     \"\"\"\n#     os.makedirs(destination_folder, exist_ok=True)\n\n#     for path in image_paths:\n#         img = cv2.imread(path)\n#         if img is None:\n#             print(f\"‚ö†Ô∏è Warning: Could not read image {path}, skipping...\")\n#             continue\n\n#         resized_img = cv2.resize(img, (width, height))  # (width, height) order\n\n#         # Build destination path\n#         filename = os.path.basename(path)\n#         dest_path = os.path.join(destination_folder, filename)\n\n#         cv2.imwrite(dest_path, resized_img)\n#         #print(f\"‚úÖ Saved resized image to {dest_path}\")\n\n# # run\n# resize_and_copy_images(folder, '/kaggle/working/Resized_Images3', min_h, min_w)\n\n# # print number of images in resized_image folders\n# folder2 = []\n# for dirname, _, filenames in os.walk('/kaggle/working/Resized_Images'):\n#     for filename in filenames:\n#         folder2.append(os.path.join(dirname, filename))\n# print(len(folder2))","metadata":{"_uuid":"4cedb7c9-435c-4ebb-b8e4-3312d6772066","_cell_guid":"f292c467-5c8d-405a-8a19-f289778ca31b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:54:07.056274Z","iopub.execute_input":"2025-10-19T12:54:07.056501Z","iopub.status.idle":"2025-10-19T12:54:07.066715Z","shell.execute_reply.started":"2025-10-19T12:54:07.056477Z","shell.execute_reply":"2025-10-19T12:54:07.065894Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import shutil\nimport cv2\n\ndef move_data(image_paths, destination_folder):\n    os.makedirs(destination_folder, exist_ok=True)\n    processed_files = []\n\n    moved_count = 0\n    missing_count = 0\n\n    for img_path in image_paths:\n        img = cv2.imread(img_path)\n        base_name = os.path.splitext(os.path.basename(img_path))[0]\n        upscaled_path = os.path.join(destination_folder, f\"{base_name}.jpg\")\n       \n        if os.path.exists(destination_folder):\n            cv2.imwrite(upscaled_path, img)\n            moved_count += 1\n        else:\n            missing_count += 1\n\n        processed_files.append(upscaled_path)\n    print(f\"count: {moved_count} images moved, {missing_count} missing\")\n    return processed_files\n\nprocessed_files = move_data(folder, '/kaggle/working/Resized_Images3')\nprint(len(processed_files))","metadata":{"_uuid":"51aace63-6248-4e52-8abe-10a406557b7d","_cell_guid":"d19be12e-1e09-47fe-90a2-fb43dfe26be9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:54:07.067688Z","iopub.execute_input":"2025-10-19T12:54:07.067946Z","iopub.status.idle":"2025-10-19T12:55:19.307201Z","shell.execute_reply.started":"2025-10-19T12:54:07.067928Z","shell.execute_reply":"2025-10-19T12:55:19.306315Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"count: 877 images moved, 0 missing\n877\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# !pip install realesrgan","metadata":{"_uuid":"9d3c9414-8b46-44e5-b437-d384ddadfb7e","_cell_guid":"92e1e922-03be-4e0d-830f-024d7b41813a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:19.308060Z","iopub.execute_input":"2025-10-19T12:55:19.308399Z","iopub.status.idle":"2025-10-19T12:55:19.312049Z","shell.execute_reply.started":"2025-10-19T12:55:19.308377Z","shell.execute_reply":"2025-10-19T12:55:19.311209Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# import os\n# import cv2\n# # from realesrgan import RealESRGAN\n# from PIL import Image\n# import torch\n\n# def enhance_cadastral_images(image_paths, destination_folder):\n#     \"\"\"\n#     Enhance and upscale cadastral images for clearer OCR extraction.\n    \n#     Args:\n#         image_paths (list): List of image file paths to process.\n#         destination_folder (str): Folder path to save processed images.\n    \n#     Returns:\n#         list: List of file paths to the processed images.\n#     \"\"\"\n#     # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#     # model = RealESRGAN(device, scale=4)\n#     # model.load_weights('RealESRGAN_x4plus.pth')\n#     # final = '/kaggle/working/Resized_Images3'\n#     os.makedirs(destination_folder, exist_ok=True)\n#     # os.makedirs(final, exist_ok=True)\n#     processed_files = []\n#     count = 0\n    \n#     for img_path in image_paths:\n#         # Step 1 ‚Äî Load and upscale\n#         # img = Image.open(img_path)\n#         # upscaled = model.predict(img)\n        \n#         # base_name = os.path.splitext(os.path.basename(img_path))[0]\n#         # upscaled_path = os.path.join(destination_folder, f\"{base_name}.png\")\n#         # upscaled.save(upscaled_path)\n\n#         img = cv2.imread(img_path)\n#         upscaled = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n\n#         base_name = os.path.splitext(os.path.basename(img_path))[0]\n#         upscaled_path = os.path.join(destination_folder, f\"{base_name}.jpg\")\n#         # cv2.imwrite(upscaled_path, upscaled)\n\n#         # # Step 2 ‚Äî Convert for OpenCV\n#         # img_cv = cv2.imread(upscaled_path)\n\n#         # Step 3 ‚Äî Denoise and sharpen\n#         # denoised = cv2.fastNlMeansDenoisingColored(upscaled, None, 10, 10, 7, 21) # extremely slow\n#         denoised = upscaled\n#         blur = cv2.GaussianBlur(denoised, (3, 3), 0)\n#         sharpened = cv2.addWeighted(denoised, 1.5, blur, -0.5, 0)\n#         # enhanced = cv2.detailEnhance(sharpened, sigma_s=10, sigma_r=0.15) # runs slowly\n\n\n#         # Step 4 ‚Äî Convert to grayscale and enhance contrast\n#         gray = cv2.cvtColor(sharpened, cv2.COLOR_BGR2GRAY)\n#         # gray = cv2.cvtColor(enhanced, cv2.COLOR_BGR2GRAY) # runs slowly\n#         clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n#         equalized = clahe.apply(gray)\n\n#         # Step 5 ‚Äî Adaptive threshold (for OCR)\n#         thresh = cv2.adaptiveThreshold(\n#             equalized, 255,\n#             cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n#             cv2.THRESH_BINARY, 15, 9\n#         )\n\n#         # final_path = os.path.join(final, f\"{base_name}.png\")\n#         cv2.imwrite(upscaled_path, thresh)\n#         processed_files.append(upscaled_path)\n#         # count = count + 1\n#         # print(count)\n\n#     return processed_files\n\n# # run function\n# processed_files = enhance_cadastral_images(folder, '/kaggle/working/Resized_Images3')\n\n# # print number of images in resized_image folders\n# folder2 = []\n# for dirname, _, filenames in os.walk('/kaggle/working/Resized_Images3'):\n#     for filename in filenames:\n#         folder2.append(os.path.join(dirname, filename))\n# print(len(folder2))\n# print(len(processed_files))","metadata":{"_uuid":"7a64c88e-8a6c-4d60-94f9-c4f4cfa8b2c3","_cell_guid":"58ba8400-8ff5-43cf-a603-7611330a043e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:19.312970Z","iopub.execute_input":"2025-10-19T12:55:19.313203Z","iopub.status.idle":"2025-10-19T12:55:19.325223Z","shell.execute_reply.started":"2025-10-19T12:55:19.313186Z","shell.execute_reply":"2025-10-19T12:55:19.324525Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Load Image","metadata":{"_uuid":"2e714aa8-7e01-4adf-97bd-bd4ff49b38fa","_cell_guid":"d7757e93-76dc-4596-be34-b03cb539a6b0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"_uuid":"1afd05e8-d4ac-40aa-8f02-885a95523f7b","_cell_guid":"93520f36-4eb8-4f50-8e95-a2920569b261","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:19.329930Z","iopub.execute_input":"2025-10-19T12:55:19.330243Z","iopub.status.idle":"2025-10-19T12:55:19.871608Z","shell.execute_reply.started":"2025-10-19T12:55:19.330217Z","shell.execute_reply":"2025-10-19T12:55:19.870803Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# from google.colab.patches import cv2_imshow\n\n# img_dir = '/kaggle/working/Resized_Images3'\n# # img_dir = '/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans'\n# # image_path = folder2[0]\n# image_path = folder[0]\n# img_path = os.path.join(img_dir, image_path)\n# image = cv2.imread(img_path)\n# if image is None:\n#     raise FileNotFoundError(f\"Image not found at {img_path}\")\n# rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # For Detectron2 Visualizer and PIL/Tesseract\n\n# print(f\"Loaded image: {img_path} with dimensions {image.shape[1]}x{image.shape[0]}\")\n\n\n# id_list2 = []\n# for x in folder:\n#     value = x.split('/')[-1].split('_')[1].split('.')[0]\n#     id_list2.append(value)\n# print(train[train['ID'] == id_list2[0]])\n# print('')\n# print('')\n\n# cv2_imshow(image)","metadata":{"_uuid":"e9f3646c-484b-45ee-9c89-19d4aac44355","_cell_guid":"0274ffad-4142-4804-97be-0d0e13cd2f12","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:19.872368Z","iopub.execute_input":"2025-10-19T12:55:19.872624Z","iopub.status.idle":"2025-10-19T12:55:19.887986Z","shell.execute_reply.started":"2025-10-19T12:55:19.872606Z","shell.execute_reply":"2025-10-19T12:55:19.887191Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# 43,328. 49mE\n# 68,609,06mN\n\n# POLYGON Z ((43311.90000000037 68630.8900000006 -0.0000234999965869, 43354.243300000206 68663.08720000088 -0.0000234999965869, 43370.805399999954 68641.21440000087 -0.0000234999965869, 43349.04000000004 68624.6799999997 -0.0000234999965869, 43328.5 68609.06000000052 -0.0000234999965869, 43311.90000000037 68630.8900000006 -0.0000234999965869))\n\n# # \"[(40621.893100373105, 66595.8724605032), (38302.022589411565, 67036.40313692056), (36941.73360799215, 68061.386011003), (38431.45285995938, 70211.92628833858), (40755.79670912793, 70892.62641527664), (42573.800161207066, 69796.02693591875), (42097.093919946885, 67990.40164147476), (40621.893100373105, 66595.8724605032)]\"\n\n# # Explanation\n\n# # The geometric location provided above represents the precise boundary of the property identified by Land Tax Number 78.05.01.016.\n\n# # Data Source: This geometry is derived from the Barbados National Digital Parcel Fabric, a comprehensive cadastral map managed by the Lands and Surveys Department of Barbados. The Land Tax Number is the unique identifier used to locate specific parcels within this official government database.\n\n# # Format: The output is a list of coordinate pairs (tuples), where each pair represents a vertex (corner) of the polygon that defines the land parcel. The list begins and ends with the same coordinate to close the polygon.\n\n# # Coordinate System: These coordinates are expressed in the Barbados National Grid, a projected coordinate system used for mapping and surveying within the country. The values represent Easting (x) and Northing (y) in meters.\n\n# # Property Details: This geometry corresponds to Lot 60 Long Bay in the parish of St. Philip, with a recorded total area of 1458.8 square meters.","metadata":{"_uuid":"4216dacf-741f-48a3-8ac3-9ef30d83419f","_cell_guid":"3122403a-aa34-4d01-809f-22ae32353367","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:19.888806Z","iopub.execute_input":"2025-10-19T12:55:19.889096Z","iopub.status.idle":"2025-10-19T12:55:19.899027Z","shell.execute_reply.started":"2025-10-19T12:55:19.889070Z","shell.execute_reply":"2025-10-19T12:55:19.898269Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Split data into directories\nimage_path = '/kaggle/working/Resized_Images3'\n# image_path = '/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans'\n\ndef split_data(df, data_type):\n    # data_dir = os.path.join(config.EXTRACT_PATH, data_type)\n    data_dir = os.path.join(image_path, data_type)\n    os.makedirs(data_dir, exist_ok=True)\n\n    moved_count = 0\n    missing_count = 0\n\n    for _, row in df.iterrows():\n        id_value = row['ID']\n        image_name = f'anonymised_{id_value}.jpg'\n        source_path = os.path.join(image_path, image_name)\n\n        if os.path.exists(source_path):\n            dest_path = os.path.join(data_dir, image_name)\n            os.rename(source_path, dest_path)\n            moved_count += 1\n        else:\n            missing_count += 1\n\n    print(f\"{data_type.capitalize()}: {moved_count} images moved, {missing_count} missing\")\n    return moved_count, missing_count\n\n# Split train and test\ntrain_moved, train_missing = split_data(train, 'train')\ntest_moved, test_missing = split_data(test, 'test')\n\nprint(f\"\\nSummary:\")\nprint(f\"Training images: {train_moved}\")\nprint(f\"Test images: {test_moved}\")\nprint(f\"Total moved: {train_moved + test_moved}\")","metadata":{"_uuid":"cc5475e9-0031-4839-90f8-9e732769c83b","_cell_guid":"4ab0ed64-6af6-4f3c-be58-9eed0e010993","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:19.899957Z","iopub.execute_input":"2025-10-19T12:55:19.900229Z","iopub.status.idle":"2025-10-19T12:55:20.299256Z","shell.execute_reply.started":"2025-10-19T12:55:19.900205Z","shell.execute_reply":"2025-10-19T12:55:20.298297Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Train: 658 images moved, 308 missing\nTest: 219 images moved, 0 missing\n\nSummary:\nTraining images: 658\nTest images: 219\nTotal moved: 877\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# check for duplicates in train_df using ID columns\nprint(\"total duplicates in training data:\", train.duplicated(subset='ID').sum())\n\n# drop duplicates\n# train.drop_duplicates(subset='ID', inplace=True)\n\nprint(train.shape)\ntrain = train[~train.duplicated(subset='ID', keep=False)]\nprint(train.shape)","metadata":{"_uuid":"dcae0a86-2703-4e3a-a947-c5356b7a2ed6","_cell_guid":"95cb5f54-9910-4cd6-b272-a0c8977614f1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.299964Z","iopub.execute_input":"2025-10-19T12:55:20.300154Z","iopub.status.idle":"2025-10-19T12:55:20.307425Z","shell.execute_reply.started":"2025-10-19T12:55:20.300139Z","shell.execute_reply":"2025-10-19T12:55:20.306570Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"total duplicates in training data: 304\n(966, 10)\n(588, 10)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Prepare training dataset\ndef prepare_training_dataset():\n    \"\"\"Prepare and format training dataset\"\"\"\n    # Clean training data\n    train_clean = train.copy()\n    # train_clean.drop(\"geometry\", axis=1, inplace=True)\n    train_clean.drop_duplicates(inplace=True)\n\n    # Create target JSON\n    def create_target_json(row):\n        target_data = {\n            \"Land Surveyor\": row.get(\"Land Surveyor\"),\n            \"Surveyed For\": row.get(\"Surveyed For\"),\n            \"Certified date\": row.get(\"Certified date\"),\n            \"Total Area\": str(row.get(\"Total Area\")) if row.get(\"Total Area\") is not None else None,\n            \"Unit of Measurement\": row.get(\"Unit of Measurement\"),\n            \"Address\": row.get(\"Address\"),\n            \"Parish\": row.get(\"Parish\"),\n            \"LT Num\": row.get(\"LT Num\"),\n            \"Survey-Grade WKT Polygon\": row.get(\"geometry\")\n        }\n        # Convert NaN values to None for JSON null\n        for key, value in target_data.items():\n            if pd.isna(value) or value == '':\n                target_data[key] = None\n        return json.dumps(target_data)\n\n    train_clean['text'] = train_clean.apply(create_target_json, axis=1)\n    train_clean = train_clean[['ID', 'text']]\n\n    # Merge with image paths\n    train_image_dir = os.path.join(image_path, 'train')\n    train_image_files = [f for f in os.listdir(train_image_dir) if f.endswith('.jpg')]\n\n    train_image_df = pd.DataFrame({\n        'ID': [f.replace('anonymised_', '').replace('.jpg', '') for f in train_image_files],\n        'image_path': [os.path.join(train_image_dir, f) for f in train_image_files]\n    })\n\n    # Merge datasets\n    train_dataset = pd.merge(train_clean, train_image_df, on='ID', how='inner')\n    return train_dataset\n\n# Prepare test dataset\ndef prepare_test_dataset():\n    \"\"\"Prepare and format training dataset\"\"\"\n    # Clean test data\n    test_clean = test.copy()\n\n    # Create target JSON\n    def create_target_json(row):\n        target_data = {\n            \"Land Surveyor\": row.get(\"Land Surveyor\"),\n            \"Surveyed For\": row.get(\"Surveyed For\"),\n            \"Certified date\": row.get(\"Certified date\"),\n            \"Total Area\": str(row.get(\"Total Area\")) if row.get(\"Total Area\") is not None else None,\n            \"Unit of Measurement\": row.get(\"Unit of Measurement\"),\n            \"Address\": row.get(\"Address\"),\n            \"Parish\": row.get(\"Parish\"),\n            \"LT Num\": row.get(\"LT Num\"),\n            \"Survey-Grade WKT Polygon\": row.get(\"geometry\")\n        }\n        # Convert NaN values to None for JSON null\n        for key, value in target_data.items():\n            if pd.isna(value) or value == '':\n                target_data[key] = None\n        return json.dumps(target_data)\n\n    test_clean['text'] = test_clean.apply(create_target_json, axis=1)\n    test_clean = test_clean[['ID', 'text']]\n\n    # Merge with image paths\n    test_image_dir = os.path.join(image_path, 'test')\n    test_image_files = [f for f in os.listdir(test_image_dir) if f.endswith('.jpg')]\n\n    test_image_df = pd.DataFrame({\n        'ID': [f.replace('anonymised_', '').replace('.jpg', '') for f in test_image_files],\n        'image_path': [os.path.join(test_image_dir, f) for f in test_image_files]\n    })\n\n    # Merge datasets\n    test_dataset = pd.merge(test_clean, test_image_df, on='ID', how='inner')\n    return test_dataset\n\ntrain_dataset = prepare_training_dataset()\ntest_dataset = prepare_test_dataset()\n# display(train_dataset)\ntrain_dataset.head()","metadata":{"_uuid":"f1781d95-696c-46bf-a933-4ea2fe7a22af","_cell_guid":"264e30aa-1cb8-4abb-b0a5-6be96f0155dc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.308514Z","iopub.execute_input":"2025-10-19T12:55:20.309382Z","iopub.status.idle":"2025-10-19T12:55:20.375683Z","shell.execute_reply.started":"2025-10-19T12:55:20.309328Z","shell.execute_reply":"2025-10-19T12:55:20.374989Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"         ID  \\\n0  7703-079   \n1  7703-062   \n2  7704-115   \n3  7704-116   \n4  8603-121   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n0  {\"Land Surveyor\": \"Jennifer Alleyne\", \"Surveyed For\": \"Rico R Wiltshire\", \"Certified date\": \"2020-09-17\", \"Total Area\": \"596.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 468 Ruby Development Stage 3, Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.09.068\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42496.26640000008 69380.3217999991 -0.0000234999965869, 42494.49780000001 69378.6406999994 -0.0000234999965869, 42494.49469999969 69378.64420000091 -0.0000234999965869, 42489.30350000039 69384.20769999921 -0.0000234999965869, 42475.120699999854 69399.40770000033 -0.0000234999965869, 42486.48699999973 69410.06609999947 -0.0000234999965869, 42490.46140000038 69413.79209999926 -0.0000234999965869, 42492.08449999988 69412.05220000073 -0.0000234999965869, 42493.12150000036 69410.94089999981 -0.0000234999965869, 42509.74000000022 69393.13000000082 -0.0000234999965869, 42497.498499999754 69381.49300000072 -0.0000234999965869, 42497.367999999784 69381.36900000088 -0.0000234999965869, 42496.26640000008 69380.3217999991 -0.0000234999965869))\"}   \n1                                                                                                                            {\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Sylvia Wilson\", \"Certified date\": \"2018-10-01\", \"Total Area\": \"475.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 588 Ruby Development Stage 3 Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.07.049\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42615.50050000008 69154.04329999909 -0.0000234999965869, 42613.237499999814 69151.30089999922 -0.0000234999965869, 42595.53320000041 69166.15110000037 -0.0000234999965869, 42594.23649999965 69167.2388000004 -0.0000234999965869, 42591.93699999992 69169.16479999945 -0.0000234999965869, 42597.628600000404 69176.06959999911 -0.0000234999965869, 42602.7993999999 69182.34259999916 -0.0000234999965869, 42604.88329999987 69180.5887000002 -0.0000234999965869, 42606.482200000435 69179.24300000072 -0.0000234999965869, 42624.0700000003 69164.43999999948 -0.0000234999965869, 42615.50050000008 69154.04329999909 -0.0000234999965869))\"}   \n2                                                                                                                                                                                                                          {\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Frank Ivy\", \"Certified date\": \"2017-07-06\", \"Total Area\": \"8432.7\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Belair\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.04.01.013\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42905.95720000006 68481.60590000078 -0.0000234999965869, 42867.59159999993 68449.44889999926 -0.0000234999965869, 42845.98190000001 68483.85390000045 -0.0000234999965869, 42787.966000000015 68576.2212000005 -0.0000234999965869, 42782.200000000186 68587.1400000006 -0.0000234999965869, 42798.009899999946 68604.0764000006 -0.0000234999965869, 42798.02209999971 68604.0895000007 -0.0000234999965869, 42818.69000000041 68626.23000000045 -0.0000234999965869, 42862.37000000011 68553.8599999994 -0.0000234999965869, 42905.95720000006 68481.60590000078 -0.0000234999965869))\"}   \n3                                                                                                                                                                                                               {\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Anderson Jones\", \"Certified date\": \"2018-04-02\", \"Total Area\": \"2112.6\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 1 Belair\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.04.01.013\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42905.95720000006 68481.60590000078 -0.0000234999965869, 42867.59159999993 68449.44889999926 -0.0000234999965869, 42845.98190000001 68483.85390000045 -0.0000234999965869, 42787.966000000015 68576.2212000005 -0.0000234999965869, 42782.200000000186 68587.1400000006 -0.0000234999965869, 42798.009899999946 68604.0764000006 -0.0000234999965869, 42798.02209999971 68604.0895000007 -0.0000234999965869, 42818.69000000041 68626.23000000045 -0.0000234999965869, 42862.37000000011 68553.8599999994 -0.0000234999965869, 42905.95720000006 68481.60590000078 -0.0000234999965869))\"}   \n4                                                                                                                                                                                                                                                                                                                              {\"Land Surveyor\": \"Michelle St.Clair\", \"Surveyed For\": \"McCollin Hendy\", \"Certified date\": \"2023-10-11\", \"Total Area\": \"598.4\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Kirtons\", \"Parish\": \"St. Philip\", \"LT Num\": \"86.03.01.019\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((39989.26090000011 66736.33420000039 -0.0000234999965869, 39989.101999999955 66734.86339999922 -0.0000234999965869, 39987.09069999959 66716.24660000019 -0.0000234999965869, 39987.08999999985 66716.24000000022 -0.0000234999965869, 39962.031100000255 66717.92669999972 -0.0000234999965869, 39964.02109999955 66736.81900000013 -0.0000234999965869, 39964.188599999994 66738.40919999965 -0.0000234999965869, 39989.26090000011 66736.33420000039 -0.0000234999965869))\"}   \n\n                                                      image_path  \n0  /kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg  \n1  /kaggle/working/Resized_Images3/train/anonymised_7703-062.jpg  \n2  /kaggle/working/Resized_Images3/train/anonymised_7704-115.jpg  \n3  /kaggle/working/Resized_Images3/train/anonymised_7704-116.jpg  \n4  /kaggle/working/Resized_Images3/train/anonymised_8603-121.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>text</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7703-079</td>\n      <td>{\"Land Surveyor\": \"Jennifer Alleyne\", \"Surveyed For\": \"Rico R Wiltshire\", \"Certified date\": \"2020-09-17\", \"Total Area\": \"596.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 468 Ruby Development Stage 3, Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.09.068\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42496.26640000008 69380.3217999991 -0.0000234999965869, 42494.49780000001 69378.6406999994 -0.0000234999965869, 42494.49469999969 69378.64420000091 -0.0000234999965869, 42489.30350000039 69384.20769999921 -0.0000234999965869, 42475.120699999854 69399.40770000033 -0.0000234999965869, 42486.48699999973 69410.06609999947 -0.0000234999965869, 42490.46140000038 69413.79209999926 -0.0000234999965869, 42492.08449999988 69412.05220000073 -0.0000234999965869, 42493.12150000036 69410.94089999981 -0.0000234999965869, 42509.74000000022 69393.13000000082 -0.0000234999965869, 42497.498499999754 69381.49300000072 -0.0000234999965869, 42497.367999999784 69381.36900000088 -0.0000234999965869, 42496.26640000008 69380.3217999991 -0.0000234999965869))\"}</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7703-062</td>\n      <td>{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Sylvia Wilson\", \"Certified date\": \"2018-10-01\", \"Total Area\": \"475.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 588 Ruby Development Stage 3 Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.07.049\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42615.50050000008 69154.04329999909 -0.0000234999965869, 42613.237499999814 69151.30089999922 -0.0000234999965869, 42595.53320000041 69166.15110000037 -0.0000234999965869, 42594.23649999965 69167.2388000004 -0.0000234999965869, 42591.93699999992 69169.16479999945 -0.0000234999965869, 42597.628600000404 69176.06959999911 -0.0000234999965869, 42602.7993999999 69182.34259999916 -0.0000234999965869, 42604.88329999987 69180.5887000002 -0.0000234999965869, 42606.482200000435 69179.24300000072 -0.0000234999965869, 42624.0700000003 69164.43999999948 -0.0000234999965869, 42615.50050000008 69154.04329999909 -0.0000234999965869))\"}</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_7703-062.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7704-115</td>\n      <td>{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Frank Ivy\", \"Certified date\": \"2017-07-06\", \"Total Area\": \"8432.7\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Belair\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.04.01.013\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42905.95720000006 68481.60590000078 -0.0000234999965869, 42867.59159999993 68449.44889999926 -0.0000234999965869, 42845.98190000001 68483.85390000045 -0.0000234999965869, 42787.966000000015 68576.2212000005 -0.0000234999965869, 42782.200000000186 68587.1400000006 -0.0000234999965869, 42798.009899999946 68604.0764000006 -0.0000234999965869, 42798.02209999971 68604.0895000007 -0.0000234999965869, 42818.69000000041 68626.23000000045 -0.0000234999965869, 42862.37000000011 68553.8599999994 -0.0000234999965869, 42905.95720000006 68481.60590000078 -0.0000234999965869))\"}</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_7704-115.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7704-116</td>\n      <td>{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Anderson Jones\", \"Certified date\": \"2018-04-02\", \"Total Area\": \"2112.6\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 1 Belair\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.04.01.013\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42905.95720000006 68481.60590000078 -0.0000234999965869, 42867.59159999993 68449.44889999926 -0.0000234999965869, 42845.98190000001 68483.85390000045 -0.0000234999965869, 42787.966000000015 68576.2212000005 -0.0000234999965869, 42782.200000000186 68587.1400000006 -0.0000234999965869, 42798.009899999946 68604.0764000006 -0.0000234999965869, 42798.02209999971 68604.0895000007 -0.0000234999965869, 42818.69000000041 68626.23000000045 -0.0000234999965869, 42862.37000000011 68553.8599999994 -0.0000234999965869, 42905.95720000006 68481.60590000078 -0.0000234999965869))\"}</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_7704-116.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8603-121</td>\n      <td>{\"Land Surveyor\": \"Michelle St.Clair\", \"Surveyed For\": \"McCollin Hendy\", \"Certified date\": \"2023-10-11\", \"Total Area\": \"598.4\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Kirtons\", \"Parish\": \"St. Philip\", \"LT Num\": \"86.03.01.019\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((39989.26090000011 66736.33420000039 -0.0000234999965869, 39989.101999999955 66734.86339999922 -0.0000234999965869, 39987.09069999959 66716.24660000019 -0.0000234999965869, 39987.08999999985 66716.24000000022 -0.0000234999965869, 39962.031100000255 66717.92669999972 -0.0000234999965869, 39964.02109999955 66736.81900000013 -0.0000234999965869, 39964.188599999994 66738.40919999965 -0.0000234999965869, 39989.26090000011 66736.33420000039 -0.0000234999965869))\"}</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_8603-121.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"test_dataset.head()","metadata":{"_uuid":"4aab0b27-17eb-4838-99ea-fbb6c38cf4f2","_cell_guid":"4323a69e-6ffd-424b-9792-632065b4657b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.376309Z","iopub.execute_input":"2025-10-19T12:55:20.376483Z","iopub.status.idle":"2025-10-19T12:55:20.384144Z","shell.execute_reply.started":"2025-10-19T12:55:20.376469Z","shell.execute_reply":"2025-10-19T12:55:20.383276Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"         ID  \\\n0  7703-078   \n1  8606-095   \n2  7703-064   \n3  7703-101   \n4  7707-114   \n\n                                                                                                                                                                                                        text  \\\n0  {\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}   \n1  {\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}   \n2  {\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}   \n3  {\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}   \n4  {\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}   \n\n                                                     image_path  \n0  /kaggle/working/Resized_Images3/test/anonymised_7703-078.jpg  \n1  /kaggle/working/Resized_Images3/test/anonymised_8606-095.jpg  \n2  /kaggle/working/Resized_Images3/test/anonymised_7703-064.jpg  \n3  /kaggle/working/Resized_Images3/test/anonymised_7703-101.jpg  \n4  /kaggle/working/Resized_Images3/test/anonymised_7707-114.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>text</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7703-078</td>\n      <td>{\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}</td>\n      <td>/kaggle/working/Resized_Images3/test/anonymised_7703-078.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8606-095</td>\n      <td>{\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}</td>\n      <td>/kaggle/working/Resized_Images3/test/anonymised_8606-095.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7703-064</td>\n      <td>{\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}</td>\n      <td>/kaggle/working/Resized_Images3/test/anonymised_7703-064.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7703-101</td>\n      <td>{\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}</td>\n      <td>/kaggle/working/Resized_Images3/test/anonymised_7703-101.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7707-114</td>\n      <td>{\"Land Surveyor\": null, \"Surveyed For\": null, \"Certified date\": null, \"Total Area\": null, \"Unit of Measurement\": null, \"Address\": null, \"Parish\": null, \"LT Num\": null, \"Survey-Grade WKT Polygon\": null}</td>\n      <td>/kaggle/working/Resized_Images3/test/anonymised_7707-114.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"assert \"image_path\" in train_dataset.columns, \"Expected a column named 'image_path'\"","metadata":{"_uuid":"d4ff87bb-4171-4f5e-b291-38c660e50886","_cell_guid":"fa7b5ea7-6ce8-4767-9f9d-b63b4a0cbc9e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.384774Z","iopub.execute_input":"2025-10-19T12:55:20.384980Z","iopub.status.idle":"2025-10-19T12:55:20.396259Z","shell.execute_reply.started":"2025-10-19T12:55:20.384958Z","shell.execute_reply":"2025-10-19T12:55:20.395447Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# train_dataset['image'] = train_dataset['image_path']\n\n# # # train_dataset = train_dataset.rename(columns={\"image_path\": \"image\"}).copy()\n# # # test_dataset = test_dataset.rename(columns={\"image_path\": \"image\"}).copy()\n\n# instruction = \"\"\"Put bounding boxes around all text in this image and extract the following information in a valid json format:\n# - The name of the land surveyor.\n# - The name of the client who paid for the survey.\n# - The certification date of the survey.\n# - The total area of the survey.\n# - The unit of measurement of the total area.\n# - The address of the survey.\n# - The parish of the survey.\n# - The land tax reference number of the survey.\n\n# Guidelines:\n#  - 'Certified date' value must be in the date format: YYYY-MM-DD (year-month-day).\n#  - 'Unit of Measurement' must be in \"square meter (sq.m)\" or \"hectare (ha)\".\n#  - 'Parish': There are only 11 parishes in Barbados (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\").\n#  - 'LT Num': land tax reference number can be in the formats: dd/dd/dd/ddd or dd.dd.dd.ddd or dd-dd-dd-ddd\n#  - All fields must be accounted for.\n#  - If the value of a field is missing in the image, please fill in the value with ‚Äú‚Äù.\n#  - Do not add any extra commentary or text.\n\n# Example:\n# Json output:\n# {\n# \"Land Surveyor\": \"John Doe\",\n# \"Surveyed For\": \"Jane Deer\",\n# \"Certified date\": \"2018-01-09\",\n# \"Total Area\": \"3.26\",\n# \"Unit of Measurement\": \"ha\",\n# \"Address\": \"Belair\",\n# \"Parish\": \"St. Philip\",\n# \"LT Num\": \"77/04/01/013\",\n# }\n# \"\"\"\n\n# system_prompt = \"\"\"You are an OCR-focused Vision-Language Model. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise and deterministic.\n# \"\"\"\n\n\n# def create_instruction_column(df, instruction, system_prompt):\n#     df['instruction'] = instruction\n#     df['system_prompt'] = system_prompt\n#     return df\n\n\n# train_dataset = create_instruction_column(train_dataset, instruction, system_prompt)\n# test_dataset = create_instruction_column(test_dataset, instruction, system_prompt)\n\n\n\n# train_dataset.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.397113Z","iopub.execute_input":"2025-10-19T12:55:20.397371Z","iopub.status.idle":"2025-10-19T12:55:20.407871Z","shell.execute_reply.started":"2025-10-19T12:55:20.397344Z","shell.execute_reply":"2025-10-19T12:55:20.407140Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"train_dataset['image'] = train_dataset['image_path']\ntest_dataset['image'] = test_dataset['image_path']\n\n# train_dataset = train_dataset.rename(columns={\"image_path\": \"image\"}).copy()\n# test_dataset = test_dataset.rename(columns={\"image_path\": \"image\"}).copy()\n\n# Be sure to try all 4 prompts to see what gives you the best result\n\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n# Prompt 1:\n\n\n# system_prompt = \"\"\"You are an OCR-focused Vision-Language Model specialized in cadastral-plan interpretation. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise, deterministic and reproducible.\n\n# SURVEY-GRADE POLYGON RECONSTRUCTION RULES (applies only to this field):\n# - Field name: \"Survey-Grade WKT Polygon\". Value must be a single string containing a valid `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT with coordinates in **metres**, using the plan grid/Easting (x) and Northing (y) as shown on the document. Use Z coordinate with a small numeric placeholder if no elevation is printed (Z allowed but not required by the rest of the system; include Z to match schema).\n# - **Anchor:** If the plan contains a labeled control or computed point with numeric Easting and Northing (e.g., `E=42509.74m, N=69393.13m`), use that point as one explicit vertex in the polygon (preferably the vertex corresponding to where it is drawn). If multiple computed points are labeled, use them as vertices too where they coincide with the lot boundary.\n# - **Area constraint:** The polygon must satisfy the documented \"Total Area\" from the image. The polygon area (planar Cartesian shoelace) must match the documented Total Area within a **relative error no larger than 1e-20** (i.e., 99.99999999999999999% accuracy). If the plan lists Total Area = 596.0 sq.m, your polygon area must compute to 596.0 ¬± 596.0√ó1e-20 m¬≤.\n# - **Numeric precision:** Provide coordinates with sufficient decimal places so the area constraint can be met. Use double-precision numeric formatting (at least 14‚Äì16 significant digits) for coordinates and include the Z coordinate value (use -0.0000234999965869 or another small constant if no elevation is present).\n# - **Geometric consistency:** The polygon must be closed (first point = last point exactly, character-by-character). Coordinates must be ordered consistently (clockwise or counterclockwise). Distances and bearings should be consistent with any leg labels on the plan if visible; if numeric bearings/distances are present and legible, use them exactly to compute vertex coordinates anchored to the control point(s).\n# - **If numeric leg labels are unreadable:** you may estimate segment geometry visually from the drawing but MUST still meet the area constraint and anchor to any visible control point. If the control point is absent/unreadable, compute a polygon whose centroid and scale are consistent with the plan and the documented Total Area.\n# - **If the plan is too low-resolution to derive reliable geometry, still provide your best numeric reconstruction constrained by the documented area and any visible control point(s).** Do not output an empty string for this polygon field ‚Äî always return a polygon string.\n# \"\"\"\n\n\ninstruction = \"\"\"You are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\n\n\nOUTPUT FORMAT:\nJSON\n{\n \"Land Surveyor\": \"value\",\n \"Surveyed For\": \"value\",\n \"Certified date\": \"YYYY-MM-DD\",\n \"Total Area\": \"numeric string\",\n \"Unit of Measurement\": \"sq m\" or \"ha\",\n \"Address\": \"value\",\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\n \"LT Num\": \"dd.dd.dd.ddd\",\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))\"\n}\n\"\"\"\n\n# #----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n# # Prompt 2:\n\n# system_prompt = \"\"\"You are a precise cadastral-plan reader and geospatial data extractor. Your task is to analyze a land survey document, perform Optical Character Recognition (OCR) for textual data, and calculate the property's geometry from the provided measurements. Return ONLY the JSON object requested, with no extra commentary or markdown. Be concise, accurate, and deterministic.\n\n# Rules for Geospatial Field \"Survey-Grade WKT Polygon\":\n# You must reconstruct the geospatial lot feature by circumnavigating its boundary from a chosen starting point.\n# Use all relevant metadata from the image, including any control points (E/N coordinates), bearings (e.g., 225¬∞15'), distances (in meters), side lengths, scale, and grid references (e.g., Grid North).\n# The calculation must be highly accurate, reconciling any minor misclosures in the source data to form a valid, closed polygon. The first and last vertices of the polygon must be identical.\n# The geometry must be formatted as a Well-Known Text (WKT) string, specifically a POLYGON Z, with Z-values set to 0.\n\n# \"\"\"\n\n# #----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n# # Prompt 3:\n\n# system_prompt = \"\"\"You are an OCR-focused Vision-Language Model specialized in cadastral-plan interpretation and vector-chasing reconstructions. Always follow the user instructions exactly. Return ONLY the single JSON object requested (no extra commentary or markdown). Be concise, deterministic and reproducible.\n\n# VECTOR-CHASING / SURVEY-GRADE POLYGON RULES (applies only to \"Survey-Grade WKT Polygon\")\n# - Purpose: reconstruct the lot by **circumnavigating the boundary** from a chosen starting point using an exhaustive ordered list of side bearings and distances (vector-chasing). Use control points, labeled Easting/Northing, scale, bearings (e.g., 225¬∞15'), distances (m), and any leg labels visible on the plan.\n# - Anchor: If the plan contains a labeled computed/control point (e.g., E=42509.74, N=69393.13), include that exact coordinate as a vertex where it lies on the boundary. Use any other labeled computed points as vertices when they coincide with the boundary.\n# - Input of vectors: If the plan lists a complete ordered sequence of bearings & distances for the lot boundary, use those values exactly and perform a circumnavigation (vector chase) from an appropriate starting vertex to compute all vertices.\n# - Misclosure reconciliation: If the supplied bearings/distances lead to a small misclosure, apply an industry-standard least-squares or proportional linear adjustment (distribute closure error among legs) so the polygon closes exactly. Document nothing ‚Äî just return closed coordinates.\n# - If bearings/distances are partially unreadable: use readable leg data first; for missing legs estimate geometry visually only as necessary, but still meet the numeric constraints below.\n# - Area hard-constraint: The polygon area (planar Cartesian shoelace) MUST match the documented \"Total Area\" from the image within a relative error ‚â§ 1e-20 (i.e., 99.99999999999999999% accuracy). Enforce this by numerically adjusting coordinates (within rounding limits) after closure reconciliation.\n# - Numeric precision: provide coordinates in **metres** (plan grid Easting = X, Northing = Y). Use `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT. Include Z: if the plan prints an elevation use it; otherwise use the small constant `-0.0000234999965869`. Provide at least 15 significant digits for X/Y coordinates.\n# - Geometry consistency: polygon must be closed (first point == last point exactly, character-by-character). Order vertices consistently (CW or CCW). Coordinates must be syntactically valid floats (no commas inside numbers).\n# - If no control point or numeric legs are legible: compute a polygon whose scale, centroid, and area are consistent with the plan drawing and documented Total Area. Still return a numeric POLYGON Z string ‚Äî never \"\" for this field.\n# \"\"\"\n\n# #----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n# # Prompt 4:\n\nsystem_prompt = \"\"\"You are a specialized Vision-Language Model acting as a precise cadastral-plan interpreter and geospatial data extractor. Your task is to perform OCR for textual data and numerically reconstruct the property's geometry. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise, deterministic, and reproducible.\n\nSURVEY-GRADE POLYGON RECONSTRUCTION RULES:\n- Field name must be \"Survey-Grade WKT Polygon\".\n- The value must be a string containing a valid `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT. Coordinates must be in metres, corresponding to the plan's Easting (x) and Northing (y) grid.\n- **Method:** Reconstruct the lot feature by circumnavigating its boundary from a starting point (a method known as \"vector chasing\" or traversing). Use all legible bearings, distances, and control points.\n- **Anchor:** If the plan contains a control point with numeric Easting and Northing coordinates, you MUST use it as an anchor vertex for your calculation.\n- **Area Constraint:** The polygon's final planar area (calculated via the shoelace formula) MUST match the documented \"Total Area\" from the image with a relative error no larger than 1e-20. You must adjust the vertices as needed to satisfy this constraint.\n- **Numeric Precision:** Provide coordinates with double-precision formatting (at least 14‚Äì16 significant digits) to meet the area constraint. \n- **Include Z: if the plan prints an elevation use it; otherwise use the small constant `-0.0000234999965869`.\n- **Geometric Consistency:** The polygon must be closed (the first and last points must be identical).\n- **Fallback Logic:** If control points or numeric legs are unreadable, estimate the geometry visually but you MUST still honor the Area Constraint and any visible control point anchors. Always return a valid polygon string.\n\"\"\"\n\n# #----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\ndef create_instruction_column(df, instruction, system_prompt):\n    df['instruction'] = instruction\n    df['system_prompt'] = system_prompt\n    return df\n\n\ntrain_dataset = create_instruction_column(train_dataset, instruction, system_prompt)\ntest_dataset = create_instruction_column(test_dataset, instruction, system_prompt)\n\n\n\ntrain_dataset.head(1)","metadata":{"_uuid":"b4430f27-52ce-46f4-864e-b75f8bed82bf","_cell_guid":"085344e2-17f6-4171-9255-3eb51fec5936","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.408710Z","iopub.execute_input":"2025-10-19T12:55:20.408960Z","iopub.status.idle":"2025-10-19T12:55:20.430943Z","shell.execute_reply.started":"2025-10-19T12:55:20.408945Z","shell.execute_reply":"2025-10-19T12:55:20.430169Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"         ID  \\\n0  7703-079   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n0  {\"Land Surveyor\": \"Jennifer Alleyne\", \"Surveyed For\": \"Rico R Wiltshire\", \"Certified date\": \"2020-09-17\", \"Total Area\": \"596.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 468 Ruby Development Stage 3, Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.09.068\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42496.26640000008 69380.3217999991 -0.0000234999965869, 42494.49780000001 69378.6406999994 -0.0000234999965869, 42494.49469999969 69378.64420000091 -0.0000234999965869, 42489.30350000039 69384.20769999921 -0.0000234999965869, 42475.120699999854 69399.40770000033 -0.0000234999965869, 42486.48699999973 69410.06609999947 -0.0000234999965869, 42490.46140000038 69413.79209999926 -0.0000234999965869, 42492.08449999988 69412.05220000073 -0.0000234999965869, 42493.12150000036 69410.94089999981 -0.0000234999965869, 42509.74000000022 69393.13000000082 -0.0000234999965869, 42497.498499999754 69381.49300000072 -0.0000234999965869, 42497.367999999784 69381.36900000088 -0.0000234999965869, 42496.26640000008 69380.3217999991 -0.0000234999965869))\"}   \n\n                                                      image_path  \\\n0  /kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg   \n\n                                                           image  \\\n0  /kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              instruction  \\\n0  You are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))\"\\n}\\n   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       system_prompt  \n0  You are a specialized Vision-Language Model acting as a precise cadastral-plan interpreter and geospatial data extractor. Your task is to perform OCR for textual data and numerically reconstruct the property's geometry. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise, deterministic, and reproducible.\\n\\nSURVEY-GRADE POLYGON RECONSTRUCTION RULES:\\n- Field name must be \"Survey-Grade WKT Polygon\".\\n- The value must be a string containing a valid `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT. Coordinates must be in metres, corresponding to the plan's Easting (x) and Northing (y) grid.\\n- **Method:** Reconstruct the lot feature by circumnavigating its boundary from a starting point (a method known as \"vector chasing\" or traversing). Use all legible bearings, distances, and control points.\\n- **Anchor:** If the plan contains a control point with numeric Easting and Northing coordinates, you MUST use it as an anchor vertex for your calculation.\\n- **Area Constraint:** The polygon's final planar area (calculated via the shoelace formula) MUST match the documented \"Total Area\" from the image with a relative error no larger than 1e-20. You must adjust the vertices as needed to satisfy this constraint.\\n- **Numeric Precision:** Provide coordinates with double-precision formatting (at least 14‚Äì16 significant digits) to meet the area constraint. \\n- **Include Z: if the plan prints an elevation use it; otherwise use the small constant `-0.0000234999965869`.\\n- **Geometric Consistency:** The polygon must be closed (the first and last points must be identical).\\n- **Fallback Logic:** If control points or numeric legs are unreadable, estimate the geometry visually but you MUST still honor the Area Constraint and any visible control point anchors. Always return a valid polygon string.\\n  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>text</th>\n      <th>image_path</th>\n      <th>image</th>\n      <th>instruction</th>\n      <th>system_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7703-079</td>\n      <td>{\"Land Surveyor\": \"Jennifer Alleyne\", \"Surveyed For\": \"Rico R Wiltshire\", \"Certified date\": \"2020-09-17\", \"Total Area\": \"596.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 468 Ruby Development Stage 3, Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.09.068\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42496.26640000008 69380.3217999991 -0.0000234999965869, 42494.49780000001 69378.6406999994 -0.0000234999965869, 42494.49469999969 69378.64420000091 -0.0000234999965869, 42489.30350000039 69384.20769999921 -0.0000234999965869, 42475.120699999854 69399.40770000033 -0.0000234999965869, 42486.48699999973 69410.06609999947 -0.0000234999965869, 42490.46140000038 69413.79209999926 -0.0000234999965869, 42492.08449999988 69412.05220000073 -0.0000234999965869, 42493.12150000036 69410.94089999981 -0.0000234999965869, 42509.74000000022 69393.13000000082 -0.0000234999965869, 42497.498499999754 69381.49300000072 -0.0000234999965869, 42497.367999999784 69381.36900000088 -0.0000234999965869, 42496.26640000008 69380.3217999991 -0.0000234999965869))\"}</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg</td>\n      <td>/kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg</td>\n      <td>You are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))\"\\n}\\n</td>\n      <td>You are a specialized Vision-Language Model acting as a precise cadastral-plan interpreter and geospatial data extractor. Your task is to perform OCR for textual data and numerically reconstruct the property's geometry. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise, deterministic, and reproducible.\\n\\nSURVEY-GRADE POLYGON RECONSTRUCTION RULES:\\n- Field name must be \"Survey-Grade WKT Polygon\".\\n- The value must be a string containing a valid `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT. Coordinates must be in metres, corresponding to the plan's Easting (x) and Northing (y) grid.\\n- **Method:** Reconstruct the lot feature by circumnavigating its boundary from a starting point (a method known as \"vector chasing\" or traversing). Use all legible bearings, distances, and control points.\\n- **Anchor:** If the plan contains a control point with numeric Easting and Northing coordinates, you MUST use it as an anchor vertex for your calculation.\\n- **Area Constraint:** The polygon's final planar area (calculated via the shoelace formula) MUST match the documented \"Total Area\" from the image with a relative error no larger than 1e-20. You must adjust the vertices as needed to satisfy this constraint.\\n- **Numeric Precision:** Provide coordinates with double-precision formatting (at least 14‚Äì16 significant digits) to meet the area constraint. \\n- **Include Z: if the plan prints an elevation use it; otherwise use the small constant `-0.0000234999965869`.\\n- **Geometric Consistency:** The polygon must be closed (the first and last points must be identical).\\n- **Fallback Logic:** If control points or numeric legs are unreadable, estimate the geometry visually but you MUST still honor the Area Constraint and any visible control point anchors. Always return a valid polygon string.\\n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"# Convert to Dataset Format","metadata":{"_uuid":"c36d1127-d742-4054-b193-59e19d9970a9","_cell_guid":"c03472db-86ba-40a9-80ab-908f023950dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_dataset, preserve_index=False)\ntest_dataset = Dataset.from_pandas(test_dataset, preserve_index=False)\n\ntrain_dataset","metadata":{"_uuid":"b79b650a-7719-47c1-8060-ea1e98688d02","_cell_guid":"e881e1df-182a-402e-b83d-668b651f9768","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.431707Z","iopub.execute_input":"2025-10-19T12:55:20.432001Z","iopub.status.idle":"2025-10-19T12:55:20.472041Z","shell.execute_reply.started":"2025-10-19T12:55:20.431978Z","shell.execute_reply":"2025-10-19T12:55:20.471422Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['ID', 'text', 'image_path', 'image', 'instruction', 'system_prompt'],\n    num_rows: 584\n})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"test_dataset","metadata":{"_uuid":"4194fc2a-c86e-4418-ba6b-dc0cb6150042","_cell_guid":"7b8c115a-234d-4790-9f5d-062443e28f09","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.472691Z","iopub.execute_input":"2025-10-19T12:55:20.472905Z","iopub.status.idle":"2025-10-19T12:55:20.477493Z","shell.execute_reply.started":"2025-10-19T12:55:20.472889Z","shell.execute_reply":"2025-10-19T12:55:20.476811Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['ID', 'text', 'image_path', 'image', 'instruction', 'system_prompt'],\n    num_rows: 219\n})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"train_dataset[0]['image']","metadata":{"_uuid":"859aa5f9-2ec1-4d98-9593-04d29efd4108","_cell_guid":"50955c14-c1e6-46e4-bf00-6ecdc818001c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.478233Z","iopub.execute_input":"2025-10-19T12:55:20.478604Z","iopub.status.idle":"2025-10-19T12:55:20.492028Z","shell.execute_reply.started":"2025-10-19T12:55:20.478580Z","shell.execute_reply":"2025-10-19T12:55:20.491261Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# dataset = deepcopy(train_dataset)\n\n# dataset = dataset.cast_column(\"image\", HFImage(decode=True))\n\n# print(\"First example of the prepared dataset:\")\n# print(dataset[0])  # 'image' is a PIL.Image.Image when accessed","metadata":{"_uuid":"283b55ee-15b4-4316-94b8-bef679e5a257","_cell_guid":"d2ba856d-af38-46a9-972c-5b19745c68e2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.492745Z","iopub.execute_input":"2025-10-19T12:55:20.492965Z","iopub.status.idle":"2025-10-19T12:55:20.504285Z","shell.execute_reply.started":"2025-10-19T12:55:20.492949Z","shell.execute_reply":"2025-10-19T12:55:20.503545Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# Decode images","metadata":{"_uuid":"b46bacdc-1675-42f9-8d6a-2cdce65ec8c1","_cell_guid":"55ab97a7-883c-4800-86b5-f9ea670d6b20","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"train_dataset = train_dataset.cast_column(\"image\", HFImage(decode=True))\ntest_dataset = test_dataset.cast_column(\"image\", HFImage(decode=True))\n\nprint(\"First example of the prepared dataset:\")\nprint(train_dataset[0])  # 'image' is a PIL.Image.Image when accessed","metadata":{"_uuid":"c1d35e2b-7c8a-479f-b3bd-5f11f12257b5","_cell_guid":"3ad4e019-2364-4f44-8aa8-1ba777cdf50e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.504989Z","iopub.execute_input":"2025-10-19T12:55:20.505169Z","iopub.status.idle":"2025-10-19T12:55:20.559077Z","shell.execute_reply.started":"2025-10-19T12:55:20.505155Z","shell.execute_reply":"2025-10-19T12:55:20.558230Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"First example of the prepared dataset:\n{'ID': '7703-079', 'text': '{\"Land Surveyor\": \"Jennifer Alleyne\", \"Surveyed For\": \"Rico R Wiltshire\", \"Certified date\": \"2020-09-17\", \"Total Area\": \"596.0\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 468 Ruby Development Stage 3, Part 1\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.09.068\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42496.26640000008 69380.3217999991 -0.0000234999965869, 42494.49780000001 69378.6406999994 -0.0000234999965869, 42494.49469999969 69378.64420000091 -0.0000234999965869, 42489.30350000039 69384.20769999921 -0.0000234999965869, 42475.120699999854 69399.40770000033 -0.0000234999965869, 42486.48699999973 69410.06609999947 -0.0000234999965869, 42490.46140000038 69413.79209999926 -0.0000234999965869, 42492.08449999988 69412.05220000073 -0.0000234999965869, 42493.12150000036 69410.94089999981 -0.0000234999965869, 42509.74000000022 69393.13000000082 -0.0000234999965869, 42497.498499999754 69381.49300000072 -0.0000234999965869, 42497.367999999784 69381.36900000088 -0.0000234999965869, 42496.26640000008 69380.3217999991 -0.0000234999965869))\"}', 'image_path': '/kaggle/working/Resized_Images3/train/anonymised_7703-079.jpg', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2480x3507 at 0x7ACC20D1E3D0>, 'instruction': 'You are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))\"\\n}\\n', 'system_prompt': 'You are a specialized Vision-Language Model acting as a precise cadastral-plan interpreter and geospatial data extractor. Your task is to perform OCR for textual data and numerically reconstruct the property\\'s geometry. Always follow the user instructions exactly. Return ONLY the JSON object requested (no extra commentary, no markdown). Be concise, deterministic, and reproducible.\\n\\nSURVEY-GRADE POLYGON RECONSTRUCTION RULES:\\n- Field name must be \"Survey-Grade WKT Polygon\".\\n- The value must be a string containing a valid `POLYGON Z ((x1 y1 z1, x2 y2 z2, ..., x1 y1 z1))` WKT. Coordinates must be in metres, corresponding to the plan\\'s Easting (x) and Northing (y) grid.\\n- **Method:** Reconstruct the lot feature by circumnavigating its boundary from a starting point (a method known as \"vector chasing\" or traversing). Use all legible bearings, distances, and control points.\\n- **Anchor:** If the plan contains a control point with numeric Easting and Northing coordinates, you MUST use it as an anchor vertex for your calculation.\\n- **Area Constraint:** The polygon\\'s final planar area (calculated via the shoelace formula) MUST match the documented \"Total Area\" from the image with a relative error no larger than 1e-20. You must adjust the vertices as needed to satisfy this constraint.\\n- **Numeric Precision:** Provide coordinates with double-precision formatting (at least 14‚Äì16 significant digits) to meet the area constraint. \\n- **Include Z: if the plan prints an elevation use it; otherwise use the small constant `-0.0000234999965869`.\\n- **Geometric Consistency:** The polygon must be closed (the first and last points must be identical).\\n- **Fallback Logic:** If control points or numeric legs are unreadable, estimate the geometry visually but you MUST still honor the Area Constraint and any visible control point anchors. Always return a valid polygon string.\\n'}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"num_proc = min(24, max(1, (os.cpu_count() or 4) - 1))\n\ndef _touch_image(example):\n    # Accessing the image forces decoding; we return the example unchanged.\n    _ = example[\"image\"]\n    return example\n\n# map() already shows a progress bar; we also add a desc for clarity\ntrain_dataset = train_dataset.map(\n    _touch_image,\n    num_proc=num_proc,\n    desc=f\"Decoding/validating images with {num_proc} workers\",\n    batched=True,\n    batch_size=1,\n    load_from_cache_file=True,     # reuse cache on reruns\n    writer_batch_size=8         # big, to reduce writer overhead\n)\n\ntrain_dataset.save_to_disk('/kaggle/working/model/')\ndel train_dataset\n\ntest_dataset = test_dataset.map(\n    _touch_image,\n    num_proc=num_proc,\n    desc=f\"Decoding/validating images with {num_proc} workers\",\n    batched=True,\n    batch_size=1,\n    load_from_cache_file=True,     # reuse cache on reruns\n    writer_batch_size=8         # big, to reduce writer overhead\n)\n\ntest_dataset.save_to_disk('/kaggle/working/model2/')\ndel test_dataset","metadata":{"_uuid":"e480ead1-510e-462e-a78f-c28bfcb528f1","_cell_guid":"19f29e8b-b1b6-403f-b649-fc078bf866ec","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:20.559941Z","iopub.execute_input":"2025-10-19T12:55:20.560637Z","iopub.status.idle":"2025-10-19T12:55:41.590367Z","shell.execute_reply.started":"2025-10-19T12:55:20.560615Z","shell.execute_reply":"2025-10-19T12:55:41.589671Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Decoding/validating images with 3 workers (num_proc=3):   0%|          | 0/584 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aff22d8659c42fbafdf79ed813b51b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/584 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f04fdbdef79441ba37c4481430db0c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Decoding/validating images with 3 workers (num_proc=3):   0%|          | 0/219 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8d50a2b94a4364b58a63d1bbaa0823"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/219 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f21b7f1f2e746b8810b37bff8b8e3ed"}},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"# Dataset Description\n\nThis dataset contains survey plans from Barbados.\n\nIn Barbados, all survey plans must be submitted in analog form to the Chief Surveyor's Office in the Lands and Surveys Department. Currently, these analog documents are manually captured to be entered into Barbados‚Äô Survey Plan Register, a process which is costly in terms of time and manpower, and can result in errors in data capture.\n\nThis dataset aims to provide a digital collection of these survey plans for potential use in automating the data capture process.","metadata":{"_uuid":"25798c44-7248-4cbe-abf8-ab4cce914e49","_cell_guid":"2665a49b-e2b5-4cfe-b9ba-088a79af83f6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Loading from disk","metadata":{"_uuid":"d58b4e99-a1c8-4e7a-8bcd-7e0441b288e3","_cell_guid":"22d1b53d-7c0a-47af-9e2d-6ca7b1aefc25","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from datasets import load_from_disk\ndataset = load_from_disk('/kaggle/working/model/')\ntest_dataset = load_from_disk('/kaggle/working/model2/')","metadata":{"_uuid":"8e9bc836-d8ea-468e-866d-a27ea952f2d0","_cell_guid":"3137c096-c433-48db-b60f-b54502defc58","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:41.591910Z","iopub.execute_input":"2025-10-19T12:55:41.592708Z","iopub.status.idle":"2025-10-19T12:55:41.603866Z","shell.execute_reply.started":"2025-10-19T12:55:41.592677Z","shell.execute_reply":"2025-10-19T12:55:41.603247Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# - The control point for the cadastral plan\n# - All bearings(¬∞) and distances(m)\n\n# - 'bearing'  may be in DMS or decimal degrees; return text exactly as found.\n# - 'distance_m': Distances in meters (m) as floats.\n\n# \"Control Point\": {\"E\": 42509.74, \"N\": 69393.13},\n# \"Segments\": [\n#     {\"bearing\": \"225¬∞15'\", \"distance_m\": 16.68},\n#     {\"bearing\": \"315¬∞05'\", \"distance_m\": 24.33},\n#     ...\n#   ],","metadata":{"_uuid":"843f78bb-3bd6-469d-bc3c-b0a65ab2aa68","_cell_guid":"80d444fb-c531-4eeb-b83a-6f8099e83e75","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:41.604641Z","iopub.execute_input":"2025-10-19T12:55:41.604986Z","iopub.status.idle":"2025-10-19T12:55:41.608583Z","shell.execute_reply.started":"2025-10-19T12:55:41.604968Z","shell.execute_reply":"2025-10-19T12:55:41.607898Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# instruction = \"\"\"Put bounding boxes around all text in this image and extract the following information in a valid json format:\n# - The name of the land surveyor.\n# - The name of the client who paid for the survey.\n# - The certification date of the survey.\n# - The total area of the survey.\n# - The unit of measurement of the total area.\n# - The address of the survey.\n# - The parish of the survey.\n# - The land tax reference number of the survey.\n\n# Guidelines:\n#  - 'Certified date' value must be in the date format: YYYY-MM-DD (year-month-day).\n#  - 'Unit of Measurement' must be in \"square meter (sq.m)\" or \"hectare (ha)\".\n#  - 'Parish': There are only 11 parishes in Barbados (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\").\n#  - 'LT Num': land tax reference number can be in the formats: dd/dd/dd/ddd or dd.dd.dd.ddd or dd-dd-dd-ddd\n#  - All fields must be accounted for.\n#  - If the value of a field is missing in the image, please fill in the value with ‚Äú‚Äù.\n#  - Do not add any extra commentary or text.\n\n# Example:\n# Json output:\n# {\n# \"Land Surveyor\": \"John Doe\",\n# \"Surveyed For\": \"Jane Deer\",\n# \"Certified date\": \"2018-01-09\",\n# \"Total Area\": \"3.26\",\n# \"Unit of Measurement\": \"ha\",\n# \"Address\": \"Belair\",\n# \"Parish\": \"St. Philip\",\n# \"LT Num\": \"77/04/01/013\",\n# }\n# \"\"\"\n\n# # # Add conversation column\n# # def convert_to_conversation(sample):\n\n# #     # conversation = [\n# #     # { \"role\": \"user\",\n# #     #   \"content\": [{\"type\": \"text\",  \"text\": instruction}, {\"type\": \"image\", \"image\": sample[\"image\"]} ]\n# #     # },\n# #     # { \"role\": \"assistant\",\n# #     #   \"content\": [{\"type\": \"text\",  \"text\": sample[\"text\"]} ]\n# #     # },\n# #     # ]\n\n# #     conversation = [\n# #     { \"role\": \"user\",\n# #       \"content\": [{\"type\": \"text\",  \"text\": instruction}, {\"type\": \"image\"} ]\n# #     },\n# #     { \"role\": \"assistant\",\n# #       \"content\": [{\"type\": \"text\",  \"text\": sample[\"text\"]} ]\n# #     },\n# #     ]\n\n    \n# #     return conversation\n# # pass","metadata":{"_uuid":"3c914935-93a8-4a58-9c06-2411c38d314e","_cell_guid":"a140aecf-eec1-42f3-84b8-2225fae30735","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:41.609361Z","iopub.execute_input":"2025-10-19T12:55:41.609619Z","iopub.status.idle":"2025-10-19T12:55:41.620009Z","shell.execute_reply.started":"2025-10-19T12:55:41.609595Z","shell.execute_reply":"2025-10-19T12:55:41.619332Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# # 1) Ensure image column is a path, not decoded object. If it's decoded, replace with path or raw bytes.\n# # -> Skip step here if dataset already stores paths.\n\n# # 2) Create messages column lazily\n# def convert_to_conversation_single(example):\n#     return {\n#         \"messages\": [\n#             {\n#                 \"role\": \"user\",\n#                 \"content\": [\n#                     {\"type\": \"text\", \"text\": instruction},\n#                     {\"type\": \"image\", \"image\": example[\"image\"]},  # keep path or bytes\n#                 ],\n#             },\n#             {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"text\"]}]},\n#         ]\n#     }\n\n# dataset = dataset.map(convert_to_conversation_single, batched=False)\n\n# # 3) Split using HF's function (no large Python lists)\n# splits = dataset.train_test_split(test_size=0.15, seed=42)\n# train_dataset = splits[\"train\"]\n# val_dataset = splits[\"test\"]\n\n# # 4) Remove unnecessary columns to reduce memory\n# cols_to_keep = [\"messages\"]  # and any id field you need\n# train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in cols_to_keep])\n# val_dataset = val_dataset.remove_columns([c for c in val_dataset.column_names if c not in cols_to_keep])\n\n# # free memory\n# import gc, torch\n# del dataset, splits\n# gc.collect()\n# torch.cuda.empty_cache()\n\n# print(f\"Training dataset size: {len(train_dataset)}\")\n# print(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"_uuid":"458d11e5-1e21-492a-a559-29f6632946d8","_cell_guid":"f7e8b06f-d4f5-488f-b0c3-79dfdd27578e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:41.620654Z","iopub.execute_input":"2025-10-19T12:55:41.620822Z","iopub.status.idle":"2025-10-19T12:55:41.633062Z","shell.execute_reply.started":"2025-10-19T12:55:41.620807Z","shell.execute_reply":"2025-10-19T12:55:41.632439Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# Splitting into training and validation sets","metadata":{"_uuid":"9db23f58-9ac3-42dc-a3ce-780287d16e98","_cell_guid":"f4cb3998-3845-453c-b749-1501bab0e812","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Path 1","metadata":{"_uuid":"67be80a7-fff2-4e34-8e95-a497659ba40c","_cell_guid":"8121db7c-585e-41f3-85ea-45c8dae94491","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\n# # Resize to (512, 512)\n# def resize_images(example):\n#     image = example[\"decoded_image\"]\n#     image = image.resize((512, 512))\n#     example[\"decoded_image\"] = image\n#     return example\n# dataset = dataset.map(resize_images)\n\n# # Then convert to RGB\n# def convert_to_rgb(example):\n#     image = example[\"decoded_image\"]\n#     if image.mode != \"RGB\":\n#         image = image.convert(\"RGB\")\n#     example[\"decoded_image\"] = image\n#     return example\n# dataset = dataset.map(convert_to_rgb)\n\n\nclass LazyImageConversationDataset(Dataset):\n    \"\"\"\n    Loads image files lazily and constructs the conversation messages\n    structure expected by UnslothVisionDataCollator.\n    \"\"\"\n    def __init__(self, hf_dataset, instruction, system_prompt, image_column=\"image_path\", img='image', text_column=\"text\"):\n        self.ds = hf_dataset\n        self.image_column = image_column\n        self.image = img\n        self.text_column = text_column\n        self.instruction = instruction\n        self.system_prompt = system_prompt\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        sample = self.ds[idx]\n        img_path = sample[self.image_column]\n\n        # --- Load image on demand ---\n        if isinstance(img_path, bytes):\n            # In case images are stored as bytes\n            from io import BytesIO\n            image = Image.open(BytesIO(img_path)).convert(\"RGB\")\n            image = image.resize((512, 512))\n        else:\n            # Normal case: path string\n            if not os.path.exists(img_path):\n                raise FileNotFoundError(f\"Image not found: {img_path}\")\n            image = Image.open(img_path).convert(\"RGB\")\n            image = image.resize((512, 512))\n\n        # image = sample[self.image]\n\n        # # resize image\n        # image = image.resize((512, 512))\n        \n\n        # --- Build the messages list ---\n        messages = [\n            # {\n            #     \"role\": \"system\",\n            #     \"content\": [{\"type\": \"text\", \"text\": self.system_prompt}],\n            # },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": self.instruction},\n                    {\"type\": \"image\", \"image\": image},  # pass PIL Image object here\n                ],\n            },\n            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[self.text_column]}]},\n        ]\n\n        return {\"messages\": messages}\n\n# ---- Split indices instead of materializing large lists ----\nindices = list(range(len(dataset)))\ntrain_idx, val_idx = train_test_split(indices, test_size=0.05, random_state=42)\n\ntrain_hf = dataset.select(train_idx)\nval_hf = dataset.select(val_idx)\n\n# ---- Create lazy datasets ----\ntrain_dataset = LazyImageConversationDataset(train_hf, instruction, system_prompt, image_column=\"image_path\", img='image', text_column=\"text\")\nval_dataset = LazyImageConversationDataset(val_hf, instruction, system_prompt, image_column=\"image_path\", img='image', text_column=\"text\")\n\n\n# free memory\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"_uuid":"a1774280-7d26-468b-82c6-35d8aad3c14d","_cell_guid":"bb31d8b9-0e9b-4ec1-baac-93eea97012ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:41.633903Z","iopub.execute_input":"2025-10-19T12:55:41.634145Z","iopub.status.idle":"2025-10-19T12:55:42.277133Z","shell.execute_reply.started":"2025-10-19T12:55:41.634123Z","shell.execute_reply":"2025-10-19T12:55:42.276471Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Training dataset size: 554\nValidation dataset size: 30\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Path 2","metadata":{"_uuid":"314a56c3-fcba-40dd-b089-480c8baaeda6","_cell_guid":"4e02205e-5a90-4bfd-92ff-7d4d54705d50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# def convert_to_conversation(sample):\n#     conversation = [\n#     {\n#         \"role\": \"system\",\n#         \"content\": [{\"type\": \"text\", \"text\": sample[\"system_prompt\"]}],\n#     },\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\"type\": \"text\", \"text\": sample[\"instruction\"]},\n#             {\"type\": \"image\", \"image\": sample[\"image\"]},\n#         ],\n#     },\n#     {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}]},\n#     ]\n#     return {\"messages\": conversation}\n# pass\n\n# dataset = [convert_to_conversation(sample) for sample in dataset]\n\n# # Split the dataset into training and validation sets\n# train_dataset, val_dataset = train_test_split(\n#     dataset,\n#     test_size=0.15,  # 15% for validation\n#     random_state=42, # for reproducibility\n#     # You might want to add stratify here if your data has relevant categories to stratify by\n# )\n\n\n# # dataset = dataset.map(convert_to_conversation)\n\n# # dataset = dataset.train_test_split(test_size=0.15, shuffle=True, seed=42)\n\n\n# print(f\"Training dataset size: {len(train_dataset)}\")\n# print(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.277864Z","iopub.execute_input":"2025-10-19T12:55:42.278119Z","iopub.status.idle":"2025-10-19T12:55:42.282199Z","shell.execute_reply.started":"2025-10-19T12:55:42.278101Z","shell.execute_reply":"2025-10-19T12:55:42.281462Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# import os\n# import gc\n# import psutil\n# from collections import OrderedDict\n# from PIL import Image\n# from torchvision import transforms\n# from sklearn.model_selection import train_test_split\n# from torch.utils.data import Dataset\n# import torch\n# from transformers import AutoTokenizer\n\n\n# # ------------------------------------------------------------\n# # 1Ô∏è‚É£ Build data list\n# # ------------------------------------------------------------\n# data_list = []\n# for i in range(len(dataset)):\n#     data_list.append({\n#         \"image\": dataset[i][\"image_path\"],  # path or PIL object\n#         \"text\": dataset[i][\"text\"],\n#         \"instruction\": dataset[i][\"instruction\"],\n#         # Optional field if used in conversation\n#         \"system_prompt\": dataset[i].get(\"system_prompt\", \"You are a helpful assistant.\"),\n#     })\n\n\n# # ------------------------------------------------------------\n# # 2Ô∏è‚É£ Define LRU + Memory-Safe Cached Dataset\n# # ------------------------------------------------------------\n# class LazyVisionConversationDataset(Dataset):\n#     \"\"\"\n#     Lazily loads and caches preprocessed image tensors.\n#     Automatically evicts least-recently-used items when RAM usage exceeds threshold.\n#     Preserves original aspect ratio (no resizing).\n#     \"\"\"\n\n#     def __init__(self, data, tokenizer, image_root=None, device=\"cpu\",\n#                  max_cache_fraction=0.7, safety_margin_gb=2.0):\n#         self.data = data\n#         self.tokenizer = tokenizer\n#         self.image_root = image_root\n#         self.device = device\n#         self.cache = OrderedDict()\n#         self.total_bytes = 0\n#         self.max_cache_fraction = max_cache_fraction\n#         self.safety_margin_bytes = safety_margin_gb * (1024 ** 3)\n#         self.transform = transforms.ToTensor()\n\n#     def __len__(self):\n#         return len(self.data)\n\n#     def _maybe_load_image(self, image):\n#         if isinstance(image, str):\n#             path = os.path.join(self.image_root, image) if self.image_root else image\n#             img = Image.open(path).convert(\"RGB\")\n#         elif isinstance(image, Image.Image):\n#             img = image\n#         else:\n#             raise ValueError(f\"Unsupported image type: {type(image)}\")\n#         return img\n\n#     def _enough_memory(self):\n#         mem = psutil.virtual_memory()\n#         used_ratio = self.total_bytes / mem.total\n#         free_bytes = mem.available\n#         return used_ratio < self.max_cache_fraction and free_bytes > self.safety_margin_bytes\n\n#     def _evict_until_safe(self):\n#         \"\"\"Evict least-recently-used items until memory usage is safe.\"\"\"\n#         while not self._enough_memory() and len(self.cache) > 0:\n#             _, evicted_item = self.cache.popitem(last=False)\n#             image_tensor = evicted_item[\"messages\"][1][\"content\"][1][\"image\"]\n#             self.total_bytes -= image_tensor.element_size() * image_tensor.nelement()\n#             del image_tensor\n\n#     def __getitem__(self, idx):\n#         if idx in self.cache:\n#             self.cache.move_to_end(idx)\n#             return self.cache[idx]\n\n#         sample = self.data[idx]\n#         image = self._maybe_load_image(sample[\"image\"])\n#         tensor = self.transform(image)  # [C,H,W], float32\n\n#         # üß© FIX 1: Missing comma between dicts\n#         conversation = [\n#             {\n#                 \"role\": \"system\",\n#                 \"content\": [{\"type\": \"text\", \"text\": sample[\"system_prompt\"]}],\n#             },\n#             {\n#                 \"role\": \"user\",\n#                 \"content\": [\n#                     {\"type\": \"text\", \"text\": sample[\"instruction\"]},\n#                     {\"type\": \"image\", \"image\": tensor},\n#                 ],\n#             },\n#             {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}]},\n#         ]\n\n#         # üß© FIX 2: Define `item` before caching\n#         item = {\"messages\": conversation}\n\n#         # Cache management\n#         tensor_bytes = tensor.element_size() * tensor.nelement()\n#         self.total_bytes += tensor_bytes\n#         self.cache[idx] = item\n#         self.cache.move_to_end(idx)\n#         self._evict_until_safe()\n\n#         return item\n\n\n# # ------------------------------------------------------------\n# # 3Ô∏è‚É£ Split dataset\n# # ------------------------------------------------------------\n# train_data, val_data = train_test_split(data_list, test_size=0.15, random_state=42)\n\n\n# # ------------------------------------------------------------\n# # 4Ô∏è‚É£ Tokenizer & Dataset Wrappers\n# # ------------------------------------------------------------\n# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\")\n\n# train_dataset = LazyVisionConversationDataset(\n#     train_data, tokenizer, max_cache_fraction=0.7, safety_margin_gb=2.0\n# )\n# val_dataset = LazyVisionConversationDataset(\n#     val_data, tokenizer, max_cache_fraction=0.7, safety_margin_gb=2.0\n# )\n\n# # ------------------------------------------------------------\n# # 5Ô∏è‚É£ Cleanup\n# # ------------------------------------------------------------\n# gc.collect()\n# torch.cuda.empty_cache()\n\n# print(f\"Training dataset size: {len(train_dataset)}\")\n# print(f\"Validation dataset size: {len(val_dataset)}\")\n","metadata":{"_uuid":"169d549b-68e7-4172-8ef5-8d0271a04519","_cell_guid":"ff9379e6-c132-4dc8-8782-e6d576f6990c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.283110Z","iopub.execute_input":"2025-10-19T12:55:42.283664Z","iopub.status.idle":"2025-10-19T12:55:42.300183Z","shell.execute_reply.started":"2025-10-19T12:55:42.283636Z","shell.execute_reply":"2025-10-19T12:55:42.299354Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# qwen25_template = \\\n# \"\"\"{%- if tools %}\n#     {{- \\'<|im_start|>system\\\\n\\' }}\n#     {%- if messages[0][\\'role\\'] == \\'system\\' %}\n#         {{- messages[0][\\'content\\'] }}\n#     {%- else %}\n#         {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\n#     {%- endif %}\n#     {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\n#     {%- for tool in tools %}\n#         {{- \"\\\\n\" }}\n#         {{- tool | tojson }}\n#     {%- endfor %}\n#     {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\n#     {%- if messages[0][\\'role\\'] == \\'system\\' %}\n#         {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\n#     {%- else %}\n#         {{- \\'<|im_start|>system\\\\n{system_message}<|im_end|>\\\\n\\' }}\n#     {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\n#     {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n#         {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\n#     {%- elif message.role == \"assistant\" %}\n#         {{- \\'<|im_start|>\\' + message.role }}\n#         {%- if message.content %}\n#             {{- \\'\\\\n\\' + message.content }}\n#         {%- endif %}\n#         {%- for tool_call in message.tool_calls %}\n#             {%- if tool_call.function is defined %}\n#                 {%- set tool_call = tool_call.function %}\n#             {%- endif %}\n#             {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\n#             {{- tool_call.name }}\n#             {{- \\'\", \"arguments\": \\' }}\n#             {{- tool_call.arguments | tojson }}\n#             {{- \\'}\\\\n</tool_call>\\' }}\n#         {%- endfor %}\n#         {{- \\'<|im_end|>\\\\n\\' }}\n#     {%- elif message.role == \"tool\" %}\n#         {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}            {{- \\'<|im_start|>user\\' }}\n#         {%- endif %}\n#         {{- \\'\\\\n<tool_response>\\\\n\\' }}\n#         {{- message.content }}\n#         {{- \\'\\\\n</tool_response>\\' }}\n#         {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n#             {{- \\'<|im_end|>\\\\n\\' }}\n#         {%- endif %}\n#     {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\n#     {{- \\'<|im_start|>assistant\\\\n\\' }}\n# {%- endif %}\n# \"\"\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.301092Z","iopub.execute_input":"2025-10-19T12:55:42.302063Z","iopub.status.idle":"2025-10-19T12:55:42.315304Z","shell.execute_reply.started":"2025-10-19T12:55:42.302037Z","shell.execute_reply":"2025-10-19T12:55:42.314627Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Load Model","metadata":{"_uuid":"1e2304f8-8f4f-476e-bb9f-7f5a4ff870e4","_cell_guid":"28404697-e6c8-46de-81a1-180aee8c7d92","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# FastVisionModel.for_training(model) # Enable for training!\n\n# gc.collect()\n# if torch.cuda.is_available():\n#     torch.cuda.empty_cache()\n\n# trainer = SFTTrainer(\n#     model = model,\n#     tokenizer = tokenizer,\n#     data_collator = UnslothVisionDataCollator(model, tokenizer, train_on_responses_only=True, instruction_part = \"<|im_start|>user\\n\", response_part = \"<|im_start|>assistant\\n\"), # Must use!   resize = \"max\", , num_proc=4\n#     train_dataset = train_dataset,\n#     eval_dataset = val_dataset,\n#     args = SFTConfig(\n#         # per_device_train_batch_size = 8, # Increased batch size\n#         # #per_gpu_train_batch_size = 8,\n#         # gradient_accumulation_steps = 1, # Decreased accumulation steps\n\n#         per_device_train_batch_size=1,        # Small batches for memory\n#         per_device_eval_batch_size=1,\n#         gradient_accumulation_steps=16,       # Effective batch size of 16\n\n#         # warmup_steps = 5,\n#         warmup_ratio = 0.1,\n#         # max_steps = 30,\n#         num_train_epochs = 10, # Set this instead of max_steps for full training runs, 10\n#         learning_rate = 1e-4, # 5e-5\n#         fp16 = not torch.cuda.is_bf16_supported(),\n#         bf16 = torch.cuda.is_bf16_supported(),\n#         logging_steps = 10,\n#         optim = \"adamw_8bit\", # # Optimizer type (\"adam\", \"adamw\", \"adamw_torch\", \"adamw_torch_fused\", \"sgd\", \"adafactor\")  \"adamw_8bit\", \"paged_adamw_8bit\", \"paged_adamw\", \"paged_adamw_32bit\"\n#         weight_decay = 0.01,\n#         lr_scheduler_type = \"cosine\",\n#         metric_for_best_model=\"eval_loss\",\n#         load_best_model_at_end=True,\n#         seed = 3407,\n#         data_seed=5,\n#         output_dir = \"outputs\",\n#         report_to = \"none\",     # For Weights and Biases\n#         assistant_only_loss = True,\n#         # dataset_num_proc = 4,\n\n#         # You MUST put the below items for vision finetuning\n#         remove_unused_columns = False,\n#         dataloader_pin_memory=False,\n#         dataset_text_field = \"\",\n#         dataset_kwargs = {\"skip_prepare_dataset\": True},\n#         max_length = 16384,\n\n#         # # # Added to fix the error\n#         eval_strategy = \"steps\",\n#         eval_steps = 10, # You can adjust this value\n#     ),\n#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n# )","metadata":{"_uuid":"919ee21d-5e61-40d3-bcb9-8a6eb1381f1a","_cell_guid":"ed1317ec-1041-417f-95f5-bbef3277ddc4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.316144Z","iopub.execute_input":"2025-10-19T12:55:42.316378Z","iopub.status.idle":"2025-10-19T12:55:42.331341Z","shell.execute_reply.started":"2025-10-19T12:55:42.316355Z","shell.execute_reply":"2025-10-19T12:55:42.330723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# Train on responses only","metadata":{}},{"cell_type":"code","source":"# from unsloth.chat_templates import train_on_responses_only\n# trainer = train_on_responses_only(\n#     trainer,\n#     instruction_part = \"<|im_start|>system\\n\", # <|im_start|>user\\n\n#     response_part = \"<|im_start|>assistant\\n\",\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.332148Z","iopub.execute_input":"2025-10-19T12:55:42.332719Z","iopub.status.idle":"2025-10-19T12:55:42.344784Z","shell.execute_reply.started":"2025-10-19T12:55:42.332701Z","shell.execute_reply":"2025-10-19T12:55:42.344217Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"# Show memory stats","metadata":{"_uuid":"9bd5bb6d-318f-4373-bbba-4a9ccab61f91","_cell_guid":"05e99507-ae01-4ce2-8e12-6e62d65a5819","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# # @title Show current memory stats\n\n# gpu_stats = torch.cuda.get_device_properties(0)\n# start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n# max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n# print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n# print(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_uuid":"233287e4-e437-40c5-914a-d3df72a244bb","_cell_guid":"d3f34825-6b58-4bcc-af0b-3f1ead94f689","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.345543Z","iopub.execute_input":"2025-10-19T12:55:42.345772Z","iopub.status.idle":"2025-10-19T12:55:42.356750Z","shell.execute_reply.started":"2025-10-19T12:55:42.345751Z","shell.execute_reply":"2025-10-19T12:55:42.356064Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n#    \\\\   /|    Num examples = 559 | Num Epochs = 10 | Total steps = 350\n# O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n# \\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n#  \"-____-\"     Trainable parameters = 101,711,872 of 8,393,087,488 (1.21% trained)\n# Unsloth: Will smartly offload gradients to save VRAM!\n#  [101/350 1:27:13 < 3:39:22, 0.02 it/s, Epoch 2.86/10]\n# Step\tTraining Loss\tValidation Loss\n# 10\t0.715800\t0.500807\n# 20\t0.013800\t0.011555\n# 30\t0.009900\t0.007727\n# 40\t0.004100\t0.007055\n# 50\t0.007500\t0.006536\n# 60\t0.010900\t0.006215\n# 70\t0.005900\t0.005912\n# 80\t0.005700\t0.006021\n# 90\t0.005400\t0.006008\n# 100\t0.008300\t0.005706\n# 110\t0.006200\t0.005593\n# 120\t0.003500\t0.005668\n# 130\t0.004600\t0.005721\n# 140\t0.002700\t0.005469\n# 150\t0.003800\t0.005823\n# 160\t0.004500\t0.005834\n# 170\t0.002700\t0.005772\n\n","metadata":{"_uuid":"6386f8af-0e01-4e11-8b95-55c21bb1a69a","_cell_guid":"ab306267-7513-42bb-bbbb-0814bd643dcf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.357483Z","iopub.execute_input":"2025-10-19T12:55:42.357787Z","iopub.status.idle":"2025-10-19T12:55:42.368197Z","shell.execute_reply.started":"2025-10-19T12:55:42.357767Z","shell.execute_reply":"2025-10-19T12:55:42.367337Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Train Model","metadata":{"_uuid":"eba636fa-becb-4051-95a0-7f6c0403d384","_cell_guid":"ed2252e5-e3f5-4ce6-91f1-e1fa6e655dce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Step\tTraining Loss\n# 10\t0.353900\n# 20\t0.032900\n\n# Step\tTraining Loss\n# 10\t1.969700\n# 20\t0.680500\n\n# Step\tTraining Loss\n# 10\t1.479700\n# 20\t0.138600\n# 30\t0.011100\n\n# Adding training with response only to data collator\n# Step\tTraining Loss\n# 10\t0.273200\n# 20\t0.114500\n# 30\t0.105900\n\n# # Step\tTraining Loss\n# 10\t0.224300\n# 20\t0.053600\n# 30\t0.154900\n\n# # Step\tTraining Loss\n# 10\t0.134200\n# 20\t0.128300\n# 30\t0.089800\n\n# Step\tTraining Loss\tValidation Loss\n# 10\t0.694300\t0.675029\n# 20\t0.632100\t0.594522\n# 30\t0.552500\t0.521181\n# 40\t0.526900\t0.500198\n# 50\t0.502800\t0.490507\n# 60\t0.497800\t0.483690\n# 70\t0.500500\t0.478017\n# 80\t0.483000\t0.476253\n# 90\t0.483100\t0.473710\n# 100   0.481700\t0.470999\n# 110\t0.472800\t0.470643\n# 120\t0.467000\t0.470068\n# 130\t0.457500\t0.467396\n# 140\t0.465600\t0.463405\n# 150\t0.445700\t0.463261\n# 160\t0.444900\t0.462447\n# 170\t0.443000\t0.462399\n# 180\t0.440300\t0.467079\n# 190\t0.424400\t0.469531\n# 200\t0.424200\t0.471457\n","metadata":{"_uuid":"da77a899-fb9a-4165-8aa7-adb8077be33c","_cell_guid":"0a698feb-3121-40be-b5bb-812ca40fff63","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.368882Z","iopub.execute_input":"2025-10-19T12:55:42.369098Z","iopub.status.idle":"2025-10-19T12:55:42.379136Z","shell.execute_reply.started":"2025-10-19T12:55:42.369074Z","shell.execute_reply":"2025-10-19T12:55:42.378404Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# from unsloth import unsloth_train\n\n# gc.collect()\n# if torch.cuda.is_available():\n#     torch.cuda.empty_cache()\n#     torch.cuda.synchronize()\n    \n# # trainer_stats = trainer.train() << Buggy gradient accumulation\n# trainer_stats = unsloth_train(trainer)","metadata":{"_uuid":"cf846a14-8b76-40f3-b3b7-c4162c5e7866","_cell_guid":"e81927a6-627d-4bb6-bf83-d0aa45d3afc9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.379934Z","iopub.execute_input":"2025-10-19T12:55:42.380275Z","iopub.status.idle":"2025-10-19T12:55:42.391586Z","shell.execute_reply.started":"2025-10-19T12:55:42.380247Z","shell.execute_reply":"2025-10-19T12:55:42.390818Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# from ocr_evaluator import evaluate_ocr_model\n\n# # Simple evaluation\n# avg_wer, avg_cer = evaluate_ocr_model(\n#     model=your_model,\n#     processor=your_processor,\n#     dataset=your_dataset,\n#     output_dir=\"evaluation_results\"\n# )\n\n# print(f\"Average WER: {avg_wer:.4f}\")\n# print(f\"Average CER: {avg_cer:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.392288Z","iopub.execute_input":"2025-10-19T12:55:42.392717Z","iopub.status.idle":"2025-10-19T12:55:42.403704Z","shell.execute_reply.started":"2025-10-19T12:55:42.392694Z","shell.execute_reply":"2025-10-19T12:55:42.402944Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"# Inference","metadata":{"_uuid":"9a2b0fd2-436c-4d39-a38e-01794ee582f3","_cell_guid":"c593ae7c-ff36-444a-8648-608908d4aa5d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# dataset[67][\"text\"]","metadata":{"_uuid":"389d69a6-33e1-4934-9882-2ae29173c047","_cell_guid":"36db6bbc-8d97-4cf0-bd01-dbc6077b530f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.404513Z","iopub.execute_input":"2025-10-19T12:55:42.404820Z","iopub.status.idle":"2025-10-19T12:55:42.414881Z","shell.execute_reply.started":"2025-10-19T12:55:42.404787Z","shell.execute_reply":"2025-10-19T12:55:42.414135Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"class Config:\n    \"\"\"Centralized configuration for the training pipeline\"\"\"\n    DATA_PATH = \"/kaggle/working/\"\n    # EXTRACT_PATH = \"/kaggle/working/Resized_Images3\"\n    EXTRACT_PATH = \"/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans\"\n    OUTPUT_DIR = \"/kaggle/working/Barbados\"\n    HF_MODEL_NAME = \"chizurumolorondu/Qwen2.5_VL_7B-Vision_Barbados\"\n\nconfig = Config()\n\nmodel.save_pretrained(config.OUTPUT_DIR)\ntokenizer.save_pretrained(config.OUTPUT_DIR)","metadata":{"_uuid":"f6218a69-ab98-4691-8afc-bafe48c65acf","_cell_guid":"4f697b50-a9dd-4111-a82c-2cdbda0954e6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:42.415605Z","iopub.execute_input":"2025-10-19T12:55:42.415820Z","iopub.status.idle":"2025-10-19T12:55:44.579608Z","shell.execute_reply.started":"2025-10-19T12:55:42.415805Z","shell.execute_reply":"2025-10-19T12:55:44.578899Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"import gc\nimport time\n\n# del trainer_stats\n# del trainer\n# # del inputs\n# # del dataset\n# del train_dataset\n# del val_dataset\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n\n\n\n# https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl\ndef clear_memory():\n#     if \"inputs\" in globals():\n#         del globals()[\"inputs\"]\n    # if \"model\" in globals():\n    #     del globals()[\"model\"]\n    # if \"processor\" in globals():\n    #     del globals()[\"processor\"]\n    # if \"trainer\" in globals():\n    #     del globals()[\"trainer\"]\n    # if \"peft_model\" in globals():\n    #     del globals()[\"peft_model\"]\n    # if \"bnb_config\" in globals():\n    #     del globals()[\"bnb_config\"]\n    time.sleep(2)\n\n    gc.collect()\n    time.sleep(2)\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    time.sleep(2)\n    gc.collect()\n    time.sleep(2)\n\n    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n\n\nclear_memory()","metadata":{"_uuid":"c3d6e250-869a-499e-9229-15271a09994e","_cell_guid":"d1891150-f743-4864-8b88-b231f7368cb6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:44.580298Z","iopub.execute_input":"2025-10-19T12:55:44.580486Z","iopub.status.idle":"2025-10-19T12:55:54.133627Z","shell.execute_reply.started":"2025-10-19T12:55:44.580471Z","shell.execute_reply":"2025-10-19T12:55:54.132899Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"GPU allocated memory: 6.81 GB\nGPU reserved memory: 6.98 GB\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# # load adapters\n# # Now if you want to load the LoRA adapters we just saved for inference, set False to True:\n# class Config:\n#     \"\"\"Centralized configuration for the training pipeline\"\"\"\n#     DATA_PATH = \"/kaggle/working/\"\n#     # EXTRACT_PATH = \"/kaggle/working/Resized_Images3\"\n#     EXTRACT_PATH = \"/kaggle/input/barbados-lands-and-surveys-plot-automation/survey_plans\"\n#     OUTPUT_DIR = \"/kaggle/working/Barbados\"\n#     HF_MODEL_NAME = \"chizurumolorondu/Qwen2.5_VL_7B-Vision_Barbados\"\n\n# config = Config()\n\n# if True:\n#     from unsloth import FastLanguageModel\n#     model, tokenizer = FastLanguageModel.from_pretrained(\n#         model_name = '/kaggle/working/outputs/checkpoint-35', # YOUR MODEL YOU USED FOR TRAINING\n#         load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n#         max_seq_length = 2048,\n#         load_in_8bit = False,    # A bit more accurate, uses 2x memory\n#         full_finetuning = False, # We have full finetuning now!\n#         # fast_inference = True, # Enable vLLM fast inference\n#         gpu_memory_utilization = 0.8, # Reduce if out of memory\n#     )\n#     FastVisionModel.for_inference(model) # Enable for inference!\n#     # tokenizer.padding_side = \"right\"\n\n# print(f\"Before adapter parameters: {model.num_parameters()}\")\n# # model.load_adapter(adapter_name =\"/kaggle/working/Barbados\")\n# print(f\"After adapter parameters: {model.num_parameters()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T12:55:54.134377Z","iopub.execute_input":"2025-10-19T12:55:54.134651Z","iopub.status.idle":"2025-10-19T12:55:54.139246Z","shell.execute_reply.started":"2025-10-19T12:55:54.134624Z","shell.execute_reply":"2025-10-19T12:55:54.138350Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Optional: model = torch.compile(model)  # try if using PyTorch 2.x and stable for your model\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n\ndef generate_predictions(\n    model=model,\n    tokenizer=tokenizer,\n    test_dataset=test_dataset,\n    instruction=instruction,\n    batch_size=1,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n):\n    \"\"\"\n    Generate predictions for an entire dataset in small batches to control memory use.\n    Works for Hugging Face Datasets, pandas DataFrames, or lists of image paths/PIL images.\n    Streamer removed for faster, cleaner inference.\n    \"\"\"\n    # Set up for optimal performance\n    if torch.cuda.is_available():\n        # Set CUDA optimization flags\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = False\n        torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n        if hasattr(torch.backends.cudnn, \"allow_tf32\"):\n            torch.backends.cudnn.allow_tf32 = True    # Faster CUDNN ops with TF32\n\n    # Memory optimization for inference\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n    # Enable inference mode\n    FastVisionModel.for_inference(model)\n    # Enable optimizations\n    model.eval()\n    model.config.use_cache = True  # Enable KV cache for generation\n\n    # Enable inference mode for maximum optimization\n    torch._C._jit_set_profiling_executor(False)\n    torch._C._jit_set_profiling_mode(False)\n\n    # Use torch.compile if available (PyTorch 2.0+)\n    try:\n        if hasattr(torch, 'compile') and torch.cuda.is_available():\n            print(\"Using torch.compile for model acceleration\")\n            model = torch.compile(model, mode=\"reduce-overhead\")\n    except Exception as e:\n        print(f\"Torch compile not available: {e}\")\n\n    predictions = []\n\n    # Helper function to get and prepare images\n    def get_batch(dataset, start, end):\n        subset = dataset[start:end]\n\n        # Hugging Face Dataset format\n        if isinstance(subset, dict) and \"image\" in subset:\n            imgs = subset[\"image\"]\n\n        # Pandas DataFrame format\n        elif hasattr(dataset, \"iloc\"):\n            if \"image\" in dataset.columns:\n                imgs = dataset.iloc[start:end][\"image\"].tolist()\n            elif \"image_path\" in dataset.columns:\n                imgs = [Image.open(p).convert(\"RGB\").resize((512, 512)) for p in dataset.iloc[start:end][\"image_path\"]]\n            else:\n                raise ValueError(\"DataFrame must have an 'image' or 'image_path' column.\")\n\n        # List format\n        else:\n            imgs = subset\n\n        # Convert string paths to PIL images if necessary\n        if isinstance(imgs[0], str) and os.path.exists(imgs[0]):\n            imgs = [Image.open(p).convert(\"RGB\").resize((512, 512)) for p in imgs]\n\n        return imgs\n\n    # Iterate through dataset in batches\n    for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Generating predictions\"):\n        images = get_batch(test_dataset, i, i + batch_size)\n\n        # Construct messages per image\n        # messages = [\n        #     [\n        #         {\"role\": \"user\", \"content\": [\n        #             {\"type\": \"image\"},\n        #             {\"type\": \"text\", \"text\": instruction}\n        #         ]}\n        #     ] for _ in images\n        # ]\n\n        # Construct messages per image\n        messages = [\n            [\n            #     {\n            #     \"role\": \"system\",\n            #     \"content\": [{\"type\": \"text\", \"text\": system_prompt}], #test_dataset['system_prompt']\n            # },\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": instruction} #test_dataset['instruction']\n                ]}\n            ] for _ in images\n        ]\n\n        # Create input text prompts\n        input_texts = [\n            tokenizer.apply_chat_template(msg, add_generation_prompt=True)\n            for msg in messages\n        ]\n\n        # Tokenize multimodal inputs\n        inputs = tokenizer(\n            images,\n            input_texts,\n            add_special_tokens=False,\n            return_tensors=\"pt\",\n            padding=True,\n            # truncation=True,\n        ).to(device)\n\n        # Generate predictions (no streaming)\n        # with torch.no_grad():\n        with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1024,\n                use_cache=True,\n                temperature=1e-8,\n                # repetition_penalty=1.1,\n                # num_beams=1,\n                do_sample=True,\n                top_k=1,\n                top_p=1.0,\n                # no_repeat_ngram_size=3,\n                # num_beam_groups=1,\n                # min_new_tokens=20,\n                # return_dict_in_generate=True,\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n\n        # Decode full batch results at once\n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        predictions.extend(decoded)\n\n        # Clean memory per batch\n        del inputs, outputs\n        torch.cuda.empty_cache()\n\n    return predictions\n\n\n# --- Example Usage ---\ntest['pred'] = generate_predictions()","metadata":{"_uuid":"00f207a2-362d-4827-badf-87c8e41dcab2","_cell_guid":"3de2abd6-d812-497f-99a9-e11bfbc6d8b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T12:55:54.140134Z","iopub.execute_input":"2025-10-19T12:55:54.140499Z","iopub.status.idle":"2025-10-19T16:09:39.132489Z","shell.execute_reply.started":"2025-10-19T12:55:54.140470Z","shell.execute_reply":"2025-10-19T16:09:39.131721Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Using torch.compile for model acceleration\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating predictions:   0%|          | 0/219 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b464aefad134db99ba8c4dde31fefb8"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_1053/3251465760.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n/tmp/ipykernel_1053/3251465760.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.133453Z","iopub.execute_input":"2025-10-19T16:09:39.133672Z","iopub.status.idle":"2025-10-19T16:09:39.142226Z","shell.execute_reply.started":"2025-10-19T16:09:39.133654Z","shell.execute_reply":"2025-10-19T16:09:39.141496Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"         ID  \\\n0  7703-078   \n1  8606-095   \n2  7703-064   \n3  7703-101   \n4  7707-114   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         pred  \n0     system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Nero Millington\", \"Certified date\": \"2020-09-14\", \"Total Area\": \"430.8\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 636 Ruby Development; Stage 3 Part 2\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.07.015\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42711.77250  \n1                 system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Lennox J Reid\", \"Surveyed For\": \"Collymore Taylor\", \"Certified date\": \"2021-10-14\", \"Total Area\": \"428.7\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 7A Mangrove Park,\", \"Parish\": \"St. Philip\", \"LT Num\": \"86.06.07.090\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((39205.40280000017  \n2  system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Frank Frank\", \"Certified date\": \"2019-01-18\", \"Total Area\": \"661.8\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 632 Ruby Development; Stage 3 Part 2 Frank\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.08.034\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42711.700000  \n3  system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Michael L Banfield\", \"Surveyed For\": \"Smith Roachford\", \"Certified date\": \"2022-08-27\", \"Total Area\": \"527.3\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 114 Ruby Park, Ruby Development\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.02.054\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42242.86000000  \n4                system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Michael H Hutchinson\", \"Surveyed For\": \"Liam Worrell\", \"Certified date\": \"2013-02-07\", \"Total Area\": \"540.4\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 224 Union Hall\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.07.06.028\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42394.24229999985   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7703-078</td>\n      <td>system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Nero Millington\", \"Certified date\": \"2020-09-14\", \"Total Area\": \"430.8\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 636 Ruby Development; Stage 3 Part 2\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.07.015\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42711.77250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8606-095</td>\n      <td>system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Lennox J Reid\", \"Surveyed For\": \"Collymore Taylor\", \"Certified date\": \"2021-10-14\", \"Total Area\": \"428.7\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 7A Mangrove Park,\", \"Parish\": \"St. Philip\", \"LT Num\": \"86.06.07.090\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((39205.40280000017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7703-064</td>\n      <td>system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Ronald Greene\", \"Surveyed For\": \"Frank Frank\", \"Certified date\": \"2019-01-18\", \"Total Area\": \"661.8\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 632 Ruby Development; Stage 3 Part 2 Frank\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.08.034\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42711.700000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7703-101</td>\n      <td>system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Michael L Banfield\", \"Surveyed For\": \"Smith Roachford\", \"Certified date\": \"2022-08-27\", \"Total Area\": \"527.3\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 114 Ruby Park, Ruby Development\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.03.02.054\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42242.86000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7707-114</td>\n      <td>system\\nYou are a helpful assistant.\\nuser\\nYou are given an image of a land survey (cadastral) plan. Perform Optical Character Recognition (OCR) and geometry reconstruction, and return ONLY ONE JSON object exactly matching the schema below.\\n\\n\\nOUTPUT FORMAT:\\nJSON\\n{\\n \"Land Surveyor\": \"value\",\\n \"Surveyed For\": \"value\",\\n \"Certified date\": \"YYYY-MM-DD\",\\n \"Total Area\": \"numeric string\",\\n \"Unit of Measurement\": \"sq m\" or \"ha\",\\n \"Address\": \"value\",\\n \"Parish\": (\"St. Andrew\", \"St. George\", \"St. James\", \"St. John\", \"St. Joseph\", \"St. Lucy\", \"St. Michael\", \"St. Peter\", \"St. Philip\", \"St. Thomas\" and \"Christ Church\"),\\n \"LT Num\": \"dd.dd.dd.ddd\",\\n \"Survey-Grade WKT Polygon\": \"POLYGON Z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"Land Surveyor\": \"Michael H Hutchinson\", \"Surveyed For\": \"Liam Worrell\", \"Certified date\": \"2013-02-07\", \"Total Area\": \"540.4\", \"Unit of Measurement\": \"sq m\", \"Address\": \"Lot 224 Union Hall\", \"Parish\": \"St. Philip\", \"LT Num\": \"77.07.06.028\", \"Survey-Grade WKT Polygon\": \"POLYGON Z ((42394.24229999985</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"test.to_csv('new5.csv', index=False)","metadata":{"_uuid":"054a3f26-a1d3-40d8-9471-5af8f7fe5259","_cell_guid":"8ef328ab-a761-48ea-aea4-9011becc4352","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.142822Z","iopub.execute_input":"2025-10-19T16:09:39.143072Z","iopub.status.idle":"2025-10-19T16:09:39.165562Z","shell.execute_reply.started":"2025-10-19T16:09:39.143050Z","shell.execute_reply":"2025-10-19T16:09:39.165035Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# Formatting","metadata":{"_uuid":"b62e3271-a293-47db-8b62-9943d83c7113","_cell_guid":"23deaa31-146d-4e86-bccf-a63c17e584f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# import pandas as pd\n# import json\n# import numpy as np\n\n# test = pd.read_csv('/kaggle/input/barb-test3/new3.csv')\n# ss = pd.read_csv('/kaggle/input/barbados-lands-and-surveys-plot-automation/SampleSubmission.csv')","metadata":{"_uuid":"8c73cf2c-e0a8-4557-98d0-3737ddb7e6d7","_cell_guid":"c5927aee-7f24-4bbe-8c29-4efefeca8ebe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.166201Z","iopub.execute_input":"2025-10-19T16:09:39.166419Z","iopub.status.idle":"2025-10-19T16:09:39.169508Z","shell.execute_reply.started":"2025-10-19T16:09:39.166396Z","shell.execute_reply":"2025-10-19T16:09:39.168945Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"ss.head()","metadata":{"_uuid":"9ace4d03-062e-4afa-b2cc-3a38ef44bed0","_cell_guid":"ee46239d-a4c6-494f-9697-efeb45fa960d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.170255Z","iopub.execute_input":"2025-10-19T16:09:39.170483Z","iopub.status.idle":"2025-10-19T16:09:39.204749Z","shell.execute_reply.started":"2025-10-19T16:09:39.170461Z","shell.execute_reply":"2025-10-19T16:09:39.204132Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"         ID                                                    TargetSurvey  \\\n0  7703-078                andre clarke d & a developers ltd lot 1 foul bay   \n1  8606-095                   kevin belgrave keira riggs lot 217 union hall   \n2  7703-064                            andrew bannister sir alex eastbourne   \n3  7703-101  andrew r bannister paul a taylor lot 11 coral cliff bottom bay   \n4  7707-114         kary greaves the estate of jackson wilson lot 1 kirtons   \n\n  Certified date Total Area Unit of Measurement      Parish        LT Num  \\\n0     2013-11-22      411.0                sq m  St. Philip  77.03.08.014   \n1     2023-08-18      575.0                sq m  St. Philip  86.12.02.006   \n2     2017-07-18     8432.7                sq m  St. Philip  67.12.06.016   \n3     2018-09-01     1780.7                sq m  St. Philip  77.14.02.044   \n4     2019-01-11      464.5                sq m  St. Philip  55.04.02.001   \n\n                                                                                                                                                                                                                                                                                                                                                                    geometry  \\\n0                                          [(40621.893100373105, 66595.8724605032), (38302.022589411565, 67036.40313692056), (36941.73360799215, 68061.386011003), (38431.45285995938, 70211.92628833858), (40755.79670912793, 70892.62641527664), (42573.800161207066, 69796.02693591875), (42097.093919946885, 67990.40164147476), (40621.893100373105, 66595.8724605032)]   \n1                                              [(36834.740432419036, 65979.97624669618), (37414.113784933, 68036.68298551635), (38753.765833311, 71164.51187811284), (40500.98165188355, 72975.1024229841), (41566.34740067549, 72057.47822218803), (42221.1596186285, 69174.98266489034), (42076.342863686434, 68237.40126592261), (36834.740432419036, 65979.97624669618)]   \n2  [(40169.263331990966, 66802.70872313512), (38836.4224651081, 66845.64643601357), (37010.05705055735, 68629.6797001254), (37006.28549575027, 70304.27337115114), (37428.53242080681, 71628.70018211093), (38870.9196316184, 71861.57389548954), (42548.260389195864, 69284.50519962618), (42324.824878872336, 67954.25107380658), (40169.263331990966, 66802.70872313512)]   \n3                                           [(43120.913472541484, 66431.02567398369), (39440.8312946044, 68194.79112396258), (38777.29968012313, 69098.02853805388), (38619.40686759929, 72608.84553611327), (39865.62696622105, 73304.3345880736), (40513.78920238178, 72263.39533878537), (43458.82870443694, 67326.00842917852), (43120.913472541484, 66431.02567398369)]   \n4                                        [(42718.149246326546, 66183.79121446965), (37823.13926801711, 66720.75370559642), (37222.342632909116, 73263.30376793322), (38412.515091461115, 73324.74881586313), (39313.72609553286, 72724.01507559462), (42780.22671986789, 69854.26694852157), (43235.8908448038, 68152.65282262256), (42718.149246326546, 66183.79121446965)]   \n\n  Unnamed: 8  \n0             \n1             \n2             \n3             \n4             ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TargetSurvey</th>\n      <th>Certified date</th>\n      <th>Total Area</th>\n      <th>Unit of Measurement</th>\n      <th>Parish</th>\n      <th>LT Num</th>\n      <th>geometry</th>\n      <th>Unnamed: 8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7703-078</td>\n      <td>andre clarke d &amp; a developers ltd lot 1 foul bay</td>\n      <td>2013-11-22</td>\n      <td>411.0</td>\n      <td>sq m</td>\n      <td>St. Philip</td>\n      <td>77.03.08.014</td>\n      <td>[(40621.893100373105, 66595.8724605032), (38302.022589411565, 67036.40313692056), (36941.73360799215, 68061.386011003), (38431.45285995938, 70211.92628833858), (40755.79670912793, 70892.62641527664), (42573.800161207066, 69796.02693591875), (42097.093919946885, 67990.40164147476), (40621.893100373105, 66595.8724605032)]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8606-095</td>\n      <td>kevin belgrave keira riggs lot 217 union hall</td>\n      <td>2023-08-18</td>\n      <td>575.0</td>\n      <td>sq m</td>\n      <td>St. Philip</td>\n      <td>86.12.02.006</td>\n      <td>[(36834.740432419036, 65979.97624669618), (37414.113784933, 68036.68298551635), (38753.765833311, 71164.51187811284), (40500.98165188355, 72975.1024229841), (41566.34740067549, 72057.47822218803), (42221.1596186285, 69174.98266489034), (42076.342863686434, 68237.40126592261), (36834.740432419036, 65979.97624669618)]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7703-064</td>\n      <td>andrew bannister sir alex eastbourne</td>\n      <td>2017-07-18</td>\n      <td>8432.7</td>\n      <td>sq m</td>\n      <td>St. Philip</td>\n      <td>67.12.06.016</td>\n      <td>[(40169.263331990966, 66802.70872313512), (38836.4224651081, 66845.64643601357), (37010.05705055735, 68629.6797001254), (37006.28549575027, 70304.27337115114), (37428.53242080681, 71628.70018211093), (38870.9196316184, 71861.57389548954), (42548.260389195864, 69284.50519962618), (42324.824878872336, 67954.25107380658), (40169.263331990966, 66802.70872313512)]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7703-101</td>\n      <td>andrew r bannister paul a taylor lot 11 coral cliff bottom bay</td>\n      <td>2018-09-01</td>\n      <td>1780.7</td>\n      <td>sq m</td>\n      <td>St. Philip</td>\n      <td>77.14.02.044</td>\n      <td>[(43120.913472541484, 66431.02567398369), (39440.8312946044, 68194.79112396258), (38777.29968012313, 69098.02853805388), (38619.40686759929, 72608.84553611327), (39865.62696622105, 73304.3345880736), (40513.78920238178, 72263.39533878537), (43458.82870443694, 67326.00842917852), (43120.913472541484, 66431.02567398369)]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7707-114</td>\n      <td>kary greaves the estate of jackson wilson lot 1 kirtons</td>\n      <td>2019-01-11</td>\n      <td>464.5</td>\n      <td>sq m</td>\n      <td>St. Philip</td>\n      <td>55.04.02.001</td>\n      <td>[(42718.149246326546, 66183.79121446965), (37823.13926801711, 66720.75370559642), (37222.342632909116, 73263.30376793322), (38412.515091461115, 73324.74881586313), (39313.72609553286, 72724.01507559462), (42780.22671986789, 69854.26694852157), (43235.8908448038, 68152.65282262256), (42718.149246326546, 66183.79121446965)]</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"test['pred'][0].lower()","metadata":{"_uuid":"3d72853e-74ad-4eea-b4b5-1c4f20f8b207","_cell_guid":"a959e459-959f-4896-8b9e-8ead6f4bafd5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.205491Z","iopub.execute_input":"2025-10-19T16:09:39.205724Z","iopub.status.idle":"2025-10-19T16:09:39.216134Z","shell.execute_reply.started":"2025-10-19T16:09:39.205702Z","shell.execute_reply":"2025-10-19T16:09:39.215298Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"'system\\nyou are a helpful assistant.\\nuser\\nyou are given an image of a land survey (cadastral) plan. perform optical character recognition (ocr) and geometry reconstruction, and return only one json object exactly matching the schema below.\\n\\n\\noutput format:\\njson\\n{\\n \"land surveyor\": \"value\",\\n \"surveyed for\": \"value\",\\n \"certified date\": \"yyyy-mm-dd\",\\n \"total area\": \"numeric string\",\\n \"unit of measurement\": \"sq m\" or \"ha\",\\n \"address\": \"value\",\\n \"parish\": (\"st. andrew\", \"st. george\", \"st. james\", \"st. john\", \"st. joseph\", \"st. lucy\", \"st. michael\", \"st. peter\", \"st. philip\", \"st. thomas\" and \"christ church\"),\\n \"lt num\": \"dd.dd.dd.ddd\",\\n \"survey-grade wkt polygon\": \"polygon z ((x1 y1 z1, x2 y2 z2,..., x1 y1 z1))\"\\n}\\n\\nassistant\\n{\"land surveyor\": \"ronald greene\", \"surveyed for\": \"nero millington\", \"certified date\": \"2020-09-14\", \"total area\": \"430.8\", \"unit of measurement\": \"sq m\", \"address\": \"lot 636 ruby development; stage 3 part 2\", \"parish\": \"st. philip\", \"lt num\": \"77.03.07.015\", \"survey-grade wkt polygon\": \"polygon z ((42711.77250'"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"def unescape_quotes(s: str) -> str:\n    \"\"\"\n    Convert escaped quotes like \\\" or \\' into literal quotes.\n    Also handles double backslashes safely.\n    \"\"\"\n    if not isinstance(s, str):\n        return s\n    s = s.replace('\\\\\\\\', '\\\\')  # collapse double backslashes\n    s = s.replace('\\\\\"', '\"')\n    s = s.replace(\"\\\\'\", \"'\")\n    return s\n\n\ndef normalize_json_like_string(s: str) -> str:\n    \"\"\"\n    Cleans a malformed JSON-like string so it can be parsed by json.loads().\n    - Unescapes quotes\n    - Converts single-quoted keys to double quotes\n    - Ensures proper JSON formatting\n    \"\"\"\n    s = unescape_quotes(s)\n    # Replace single-quoted keys and values with double quotes safely\n    # Only replaces when quotes are not escaped and not inside double quotes\n    s = re.sub(r\"(?<!\\\\)'\", '\"', s)\n    return s\n\n# x = x.strip(\"[]'\")\n# x = x.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n# x = x.replace(\"'\", \"\")\n\n\ndef fixer(x):\n    x = x.lower()\n    x = x.replace('_', ' ').strip()\n    x = re.sub(r'\\s+', ' ', x).strip()\n    x = x.replace('\"\"}', '\"}')\n    x = x.replace(\"\\\\\", \"\")\n    x = x.replace(\"\\'\", \"'\")\n    x = x.replace(\"\\\"\", '\"')\n    x = x.replace('[', '')\n    x = x.replace(']', '')\n    x = x.replace(';', ',')\n    x = x.replace('certified data', 'certified date')\n    x = re.sub(r':\\s*(\\d+\\.\\d+\\.\\d+\\.\\d+)', r': \"\\1\"', x)\n    x = x.replace('landsurveyor', 'land surveyor')\n    x = x.replace('land survey–æ—Ä', 'land surveyor')\n    x = x.replace('surveyedfor', 'surveyed for')\n    x = x.replace('surveyed —Ñor', 'surveyed for')\n    x = x.replace('certifieddate', 'certified date')\n    x = x.replace('certified –¥–∞—Ç–µ', 'certified date')\n    x = x.replace('totalarea', 'total area')\n    x = x.replace('unitofmeasurement', 'unit of measurement')\n    x = x.replace('ltnum', 'lt num')\n    x = x.replace('total area sq m', 'total area')\n\n    x = x.replace('\\\\\\\\', '\\\\')\n    x = x.replace('\\\\\"', '\"')\n    x = x.replace(\"\\\\'\", \"'\")\n    x = normalize_json_like_string(x)\n\n    x = x.replace('parish st. phillip', 'parish\": \"st. philip')\n    x = x.replace('st. phillip', 'st. philip')\n    x = x.replace('st. philips', 'st. philip')\n    x = x.replace('st .philip', 'st. philip')\n    x = x.replace('st. philip', 'st. philip')\n    x = x.replace('st philip', 'st. philip')\n    x = x.replace('st. philp', 'st. philip')\n    x = x.replace('st. phil', 'st. philip')\n    x = x.replace('st. phili—Ä', 'st. philip')\n    x = x.replace('st. philipip', 'st. philip')\n    x = x.replace('st. philipi—Ä', 'st. philip')\n    x = x.replace('lt num validated', 'lt num')\n    x = x.replace('lt num valid', 'lt num')\n\n    try:\n        x = x.lower()\n        x = x.split('assistant')[-1].strip()\n        # x = x.split('\\n')[-1]\n\n    except:\n        x = {'land surveyor': '',\n             'surveyed for': '',\n             'certified date': '',\n             'total area': '',\n             'unit of measurement': '',\n             'address': 'nan',\n             'parish': '',\n             'lt num': '',\n             \"survey-grade wkt polygon\": ''\n             }\n\n\n\n        return x\n    else:\n        try:\n            x = json.loads(x)\n        except:\n            # val = []\n            # x = x.split(',')\n            # for y in x:\n            #     z = y.split(':')[-1].strip()\n            #     z = z.split('\"')\n            #     for a in z:\n            #         if a != '':\n            #             val.append(a)\n\n            # val = [str(x) for x in val]\n            # value = 8 - len(val)\n\n            # if value > 0:\n            #     new = list(np.zeros(value))\n            #     new = [str(x) for x in new]\n            #     val = val + new\n\n            x = {'land surveyor': '',\n             'surveyed for': '',\n             'certified date': '',\n             'total area': '',\n             'unit of measurement': '',\n             'address': 'nan',\n             'parish': '',\n             'lt num': '',\n             \"survey-grade wkt polygon\": \"\"\n             }\n\n            return x\n\n            # x = {'land surveyor': '',\n            #  'surveyed for': '',\n            #  'certified date': '',\n            #  'total area': '',\n            #  'unit of measurement': 'sq m',\n            #  'address': '',\n            #  'parish': 'st. phillip',\n            #  'lt num': ''}\n            # return x\n\n            # import ast\n            # try:\n            #     return ast.literal_eval(x)\n            # except Exception as e:\n            #     raise ValueError(f\"Cannot safely parse string to dictionary: {e}\")\n\n            # return x#x.split('{')[-1]\n        else:\n\n            # x = x.split('{')[-1]\n            return x\n\ntest['pred'] = test['pred'].apply(fixer)\ntest['pred'][0]\n\n# test['pred'].apply(fixer)[100]","metadata":{"_uuid":"d824c94e-c50d-4d5f-aab0-fb982dfd1292","_cell_guid":"e0a16610-8d34-4bc8-8fdb-f3d257714c75","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.286628Z","iopub.status.idle":"2025-10-19T16:09:39.286978Z","shell.execute_reply.started":"2025-10-19T16:09:39.286785Z","shell.execute_reply":"2025-10-19T16:09:39.286800Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['pred'][65]","metadata":{"_uuid":"b459c3ac-9fe5-4646-bea2-b03c4c44d592","_cell_guid":"b48e68be-02ec-4429-aef8-c86f4fd8a834","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.288105Z","iopub.status.idle":"2025-10-19T16:09:39.288399Z","shell.execute_reply.started":"2025-10-19T16:09:39.288238Z","shell.execute_reply":"2025-10-19T16:09:39.288254Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ['ID','Surveyed For', 'Certified date', 'Total Area',\n#        'Unit of Measurement', 'Address', 'Parish', 'LT Num', 'geometry', 'Unnamed: 8']\n\nparishes = [\n    \"st. andrew\",\n    \"st. george\",\n    \"st. james\",\n    \"st. john\",\n    \"st. joseph\",\n    \"st. lucy\",\n    \"st. michael\",\n    \"st. peter\",\n    \"st. philip\",\n    \"st. thomas\",\n    \"christ church\",\n]\n\n\ndef assigning(df):\n    value = df['pred']\n\n    # new_keys = ['land surveyor','surveyed for','certified date','total area','unit of measurement','address','parish','lt num', 'Control Point', 'Segments']\n    # # Ensure the number of new keys matches the number of existing keys\n    # if len(new_keys) == len(value):\n    #     value = dict(zip(new_keys, value.values()))\n\n    try:\n        df['Land Surveyor'] = value['land surveyor'].strip()\n    except:\n        df['Land Surveyor'] = ''\n        return df\n    else:\n        return df\n\ndef assigning2(df):\n    value = df['pred']\n    try:\n        df['Surveyed For'] = value['surveyed for'].strip()\n    except:\n        df['Surveyed For'] = ''\n        return df\n    else:\n        return df\n\ndef assigning3(df):\n    value = df['pred']\n    try:\n        df['Certified date'] = value['certified date'].strip()\n    except:\n        df['Certified date'] = ''\n        return df\n    else:\n        return df\n\ndef assigning4(df):\n    value = df['pred']\n    try:\n        df['Total Area'] = value['total area'].strip()\n    except:\n        df['Total Area'] = ''\n        return df\n    else:\n        return df\n\n\ndef assigning5(df):\n    value = df['pred']\n    try:\n        df['Unit of Measurement'] = value['unit of measurement'].strip().lower()\n    except:\n        df['Unit of Measurement'] = ''\n        return df\n    else:\n        if df['Unit of Measurement'] in ['sq.m', 'sq m', 'SQ M', 'SQ.M', 'square meters', 'square meter', 'sqm']:\n            df['Unit of Measurement'] = 'sq m'\n        elif df['Unit of Measurement'] in ['HA', 'Ha', 'ha', 'hectares', 'hectare']:\n            df['Unit of Measurement'] = 'ha'\n        return df\n\ndef assigning6(df):\n    value = df['pred']\n    try:\n        df['Address'] = value['address'].strip()\n    except:\n        df['Address'] = 'nan'\n        return df\n    else:\n        return df\n\ndef assigning7(df):\n    value = df['pred']\n    try:\n        if value['parish'] in parishes:\n            df['Parish'] = value['parish'].strip()\n        else:\n            df['Parish'] = ''\n    except:\n        df['Parish'] = ''\n        return df\n    else:\n        return df\n\ndef assigning8(df):\n    value = df['pred']\n    try:\n        df['LT Num'] = value['lt num'].strip()\n        df['LT Num'] = df['LT Num'].apply(lambda x: x.replace('\\\\', '.'))\n        df['LT Num'] = df['LT Num'].apply(lambda x: x.replace('/', '.'))\n        df['LT Num'] = df['LT Num'].apply(lambda x: x.replace('-', '.'))\n    except:\n        df['LT Num'] = ''\n        return df\n    else:\n        return df\n\n\ndef assigning9(df):\n    value = df['pred']\n    try:\n        df['geometry'] = value['survey-grade wkt polygon'].strip()\n    except:\n        df['geometry'] = ''\n        return df\n    else:\n        return df\n\n\n\ntest = test.apply(assigning, axis=1)\ntest = test.apply(assigning2, axis=1)\ntest = test.apply(assigning3, axis=1)\ntest = test.apply(assigning4, axis=1)\ntest = test.apply(assigning5, axis=1)\ntest = test.apply(assigning6, axis=1)\ntest = test.apply(assigning7, axis=1)\ntest = test.apply(assigning8, axis=1)\ntest = test.apply(assigning9, axis=1)\n\n\ntest['LT Num'] = test['LT Num'].apply(lambda x: x.replace('/', '.').strip())\ntest['LT Num'] = test['LT Num'].apply(lambda x: x.replace('//', '.').strip())\ntest['LT Num'] = test['LT Num'].apply(lambda x: x.replace('-', '.').strip())\n# test['Certified date'] = test['Certified date'].apply(lambda x: x.replace('/', '-').strip())\n# test['Certified date'] = test['Certified date'].apply(lambda x: x.replace('\\\\', '-').strip())\n\nchange = {\n    \"st. andrew\": \"St. Andrew\",\n    \"st. george\": \"St. George\",\n    \"st. james\": \"St. James\",\n    \"st. john\": \"St. John\",\n    \"st. joseph\": \"St. Joseph\",\n    \"st. lucy\": \"St. Lucy\",\n    \"st. michael\": \"St. Michael\",\n    \"st. peter\": \"St. Peter\",\n    \"st. philip\": \"St. Philip\",\n    \"st. thomas\": \"St. Thomas\",\n    \"christ church\": \"Christ Church\"\n}\ndef changes(x):\n    try:\n        x = change[x]\n    except:\n        return x\n    else:\n        return x\n\ntest['Parish'] = test['Parish'].apply(changes)\n\ntest.head()","metadata":{"_uuid":"785bace2-ce76-432d-be64-360832d4053d","_cell_guid":"1cd947e2-a0ac-4f86-af3e-29b6b3b83c67","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.289718Z","iopub.status.idle":"2025-10-19T16:09:39.290015Z","shell.execute_reply.started":"2025-10-19T16:09:39.289865Z","shell.execute_reply":"2025-10-19T16:09:39.289895Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# fix datetime","metadata":{"_uuid":"808de642-be68-4b45-9295-44f2ed0e7303","_cell_guid":"5bfa8abc-df32-4322-af1e-43f5189b9065","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Function to parse varied date strings into ISO \"yyyy-mm-dd\" format,\n# followed by tests on many example inputs.\n\n\nMONTHS = {\n    'jan':1,'january':1,\n    'feb':2,'february':2,\n    'mar':3,'march':3,\n    'apr':4,'april':4,\n    'may':5,\n    'jun':6,'june':6,\n    'jul':7,'july':7,\n    'aug':8,'august':8,\n    'sep':9,'sept':9,'september':9,\n    'oct':10,'october':10,\n    'nov':11,'november':11,\n    'dec':12,'december':12\n}\n\nCOMMON_FORMATS = [\n    \"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y.%m.%d\", \"%Y %m %d\",\n    \"%d-%m-%Y\", \"%d/%m/%Y\", \"%d.%m.%Y\", \"%d %m %Y\",\n    \"%m-%d-%Y\", \"%m/%d/%Y\", \"%m.%d.%Y\", \"%m %d %Y\",\n    \"%d %b %Y\", \"%d %B %Y\", \"%b %d %Y\", \"%B %d %Y\",\n    \"%Y%m%d\", \"%d%m%Y\", \"%m%d%Y\"\n]\n\n_ordinal_re = re.compile(r'(\\d+)(st|nd|rd|th)')\n\ndef _remove_ordinal_suffixes(s: str) -> str:\n    return _ordinal_re.sub(r'\\1', s)\n\ndef _try_common_formats(s: str):\n    for fmt in COMMON_FORMATS:\n        try:\n            dt = datetime.strptime(s, fmt)\n            return dt.date()\n        except Exception:\n            continue\n    return None\n\ndef _numbers_from_string(s: str):\n    return [int(x) for x in re.findall(r'\\d{1,4}', s)]\n\ndef parse_date_to_iso(s: str) -> str:\n    \"\"\"\n    Parse a date string containing year, month and day into ISO format \"yyyy-mm-dd\".\n    Tries multiple heuristics and common formats; accepts month names and ordinal days.\n    Raises ValueError if unable to confidently parse a year-month-day triple.\n    \"\"\"\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string\")\n    orig = s.strip()\n    if orig == \"\":\n        raise ValueError(\"Empty string\")\n\n    s = orig.strip()\n    s = s.replace(',', ' ').replace(' of ', ' ').strip()\n    s = _remove_ordinal_suffixes(s)\n    s = re.sub(r'\\s+', ' ', s)\n\n    # 1) Try datetime.fromisoformat for ISO-ish strings\n    try:\n        # fromisoformat handles YYYY-MM-DD and variants with time\n        d = datetime.fromisoformat(s)\n        return d.date().isoformat()\n    except Exception:\n        pass\n\n    # 2) Try common strptime formats\n    try_formats = _try_common_formats(s)\n    if try_formats:\n        return try_formats.isoformat()\n\n    lowered = s.lower()\n\n    # 3) If contains a month name, extract year, month, day by name/number\n    for name, num in MONTHS.items():\n        if re.search(r'\\b' + re.escape(name) + r'\\b', lowered):\n            # find year (4-digit) and day (1-2 digit)\n            year_match = re.search(r'(?P<y>\\b[12]\\d{3}\\b)', s)\n            day_match = re.search(r'\\b(\\d{1,2})\\b', s)\n            # gather all numbers\n            nums = re.findall(r'\\d{1,4}', s)\n            year = None; day = None\n            if year_match:\n                year = int(year_match.group('y'))\n                # remove year from numeric list\n                nums = [int(x) for x in nums if int(x) != year]\n            # find first numeric token that plausibly is day (1-31)\n            for token in nums:\n                if 1 <= int(token) <= 31:\n                    day = int(token)\n                    break\n            # if day missing, look for pattern like \"september 14 2020\" -> number after month\n            m_after = re.search(r'\\b' + name + r'\\b[^\\d]*(\\d{1,2})', lowered)\n            if m_after and (day is None):\n                day = int(m_after.group(1))\n            # If still missing, try numbers before month\n            if day is None:\n                m_before = re.search(r'(\\d{1,2})[^\\d]*\\b' + name + r'\\b', lowered)\n                if m_before:\n                    day = int(m_before.group(1))\n            if year is None:\n                # try to get any 4-digit\n                ym = re.search(r'\\b(1|2)\\d{3}\\b', s)\n                if ym:\n                    year = int(ym.group(0))\n            if year and day:\n                month = MONTHS[name]\n                try:\n                    dt = datetime(year, month, day).date()\n                    return dt.isoformat()\n                except Exception:\n                    raise ValueError(f\"Found tokens but invalid date from '{orig}' -> Y={year} M={month} D={day}\")\n            # fallback continue to other month name matches\n    # 4) Numeric-only heuristics\n    nums = _numbers_from_string(s)\n    if len(nums) >= 3:\n        # Prefer patterns that include a 4-digit year\n        year = next((n for n in nums if 1000 <= n <= 9999), None)\n        if year:\n            nums_no_year = nums.copy()\n            nums_no_year.remove(year)\n            # If original string starts with year -> year-month-day\n            if re.match(r'^\\s*\\d{4}', s):\n                # assume order year, month, day\n                month = nums_no_year[0]\n                day = nums_no_year[1] if len(nums_no_year) > 1 else None\n            elif re.search(r'\\d{4}\\s*$', s):\n                # year at end -> maybe day month year or month day year\n                # If first number >12 assume day first\n                first = nums_no_year[0]\n                second = nums_no_year[1] if len(nums_no_year) > 1 else None\n                if first > 12:\n                    day = first; month = second\n                else:\n                    # ambiguous: if second > 12, it's the day\n                    if second and second > 12:\n                        month = first; day = second\n                    else:\n                        # prefer day-month-year (common outside US)\n                        day = first; month = second\n            else:\n                # no clear position; try to pick month in middle if plausible\n                a, b = nums_no_year[0], nums_no_year[1] if len(nums_no_year) > 1 else None\n                if b is None:\n                    raise ValueError(f\"Not enough numeric tokens in '{orig}' to determine month/day\")\n                # choose month/day by value: if a>12 then a is day\n                if a > 12:\n                    day, month = a, b\n                else:\n                    # assume month then day\n                    month, day = a, b\n            if 1 <= month <= 12 and 1 <= day <= 31:\n                try:\n                    dt = datetime(year, month, day).date()\n                    return dt.isoformat()\n                except Exception:\n                    raise ValueError(f\"Numeric tokens gave invalid date for '{orig}' -> Y={year} M={month} D={day}\")\n    # 5) If two-digit year present (e.g., '21-03-04') it's ambiguous; avoid auto-conversion unless clear.\n    # 6) Last resort: try to find three consecutive numbers and guess ordering day/month/year if year looks like four-digit\n    trip = re.search(r'(\\d{1,4})\\D+(\\d{1,4})\\D+(\\d{1,4})', s)\n    if trip:\n        a, b, c = int(trip.group(1)), int(trip.group(2)), int(trip.group(3))\n        # identify 4-digit year among them\n        if 1000 <= a <= 9999:\n            year, month, day = a, b, c\n        elif 1000 <= b <= 9999:\n            year, month, day = b, a, c  # assume numbers around year are month/day\n        elif 1000 <= c <= 9999:\n            year, month, day = c, a, b\n        else:\n            # no 4-digit year; if one value >31 treat as year (e.g., 24-03-2024 won't reach here)\n            # give up to avoid mis-parsing\n    \n            raise ValueError(f\"Unable to confidently parse '{orig}' into year-month-day\")\n        if 1 <= month <= 12 and 1 <= day <= 31:\n            try:\n                return datetime(year, month, day).date().isoformat()\n            except Exception:\n                pass\n\n    raise ValueError(f\"Unable to parse a clear yyyy-mm-dd date from: '{orig}'\")\n\n\ndef fix_date_format(x):\n    try:\n        x = parse_date_to_iso(x)\n    except:\n        return x\n    else:\n        return x\n\ntest['Certified date'] = test['Certified date'].apply(fix_date_format)","metadata":{"_uuid":"0caa7429-9065-418f-a620-dbbf5644bed9","_cell_guid":"3f27d9df-ea1d-4e39-a82c-fd72414d2415","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.291677Z","iopub.status.idle":"2025-10-19T16:09:39.291987Z","shell.execute_reply.started":"2025-10-19T16:09:39.291871Z","shell.execute_reply":"2025-10-19T16:09:39.291885Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_target_survey(text: str) -> str:\n    \"\"\"Lowercase, remove periods and commas, normalize spaces.\"\"\"\n    text = text.lower()\n    text = re.sub(r\"[.,]\", \" \", text)      # remove periods and commas\n    text = re.sub(r\"\\s+\", \" \", text)       # normalize multiple spaces\n    return text.strip()\n\ndef format_dataset(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Adds TargetSurvey and keeps only required columns. \n    Applies lowercase, removes ., , and normalizes spaces.\n    \"\"\"\n    df[\"TargetSurvey\"] = (\n        df[\"Land Surveyor\"].astype(str).str.strip() + \" \" +\n        df[\"Surveyed For\"].astype(str).str.strip() + \" \" +\n        df[\"Address\"].astype(str).str.strip()\n    ).apply(clean_target_survey)\n\n    columns_to_keep = [\n        'ID', 'TargetSurvey', 'Certified date', 'Total Area',\n        'Unit of Measurement', 'Parish', 'LT Num', 'geometry',\n    ]\n    return df[columns_to_keep]\n\ntest = format_dataset(test)\ntest.head()","metadata":{"_uuid":"23feb97b-a68f-438d-ac3a-6de61086721c","_cell_guid":"effb44b8-1853-40c0-a0b3-f8d508e62b4b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.293351Z","iopub.status.idle":"2025-10-19T16:09:39.293587Z","shell.execute_reply.started":"2025-10-19T16:09:39.293484Z","shell.execute_reply":"2025-10-19T16:09:39.293494Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test['geometry'] = ss['geometry']\n# test['Parish'] = 'St. Philip'\n# test['Unit of Measurement'] = ss['Unit of Measurement']\ntest.to_csv('submission27.csv', index=False)","metadata":{"_uuid":"ed22f365-68c4-4cc3-af51-51f0ca0f0efe","_cell_guid":"999e7f5d-adde-459c-9a0b-c1f3d482125e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.294652Z","iopub.status.idle":"2025-10-19T16:09:39.294938Z","shell.execute_reply.started":"2025-10-19T16:09:39.294757Z","shell.execute_reply":"2025-10-19T16:09:39.294770Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['Parish'].value_counts()","metadata":{"_uuid":"a63da282-b2ea-4521-83cb-a8d457b1f9d9","_cell_guid":"0a4b7f8f-9df2-49aa-a440-1218e1a44ac8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.296018Z","iopub.status.idle":"2025-10-19T16:09:39.296310Z","shell.execute_reply.started":"2025-10-19T16:09:39.296138Z","shell.execute_reply":"2025-10-19T16:09:39.296152Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['Parish'].unique()","metadata":{"_uuid":"919150c5-e2be-4f9c-a5f0-c88d49f898bf","_cell_guid":"133909ef-e202-4a70-911f-36769c0aefa8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.297381Z","iopub.status.idle":"2025-10-19T16:09:39.298272Z","shell.execute_reply.started":"2025-10-19T16:09:39.298094Z","shell.execute_reply":"2025-10-19T16:09:39.298110Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ss['Parish'].value_counts()","metadata":{"_uuid":"c894fd64-e817-4961-a9d7-9d8b8eaa55dd","_cell_guid":"b4290276-b6b5-4941-895d-36a891b355af","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.299498Z","iopub.status.idle":"2025-10-19T16:09:39.300117Z","shell.execute_reply.started":"2025-10-19T16:09:39.299946Z","shell.execute_reply":"2025-10-19T16:09:39.299962Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ss['Unit of Measurement'].value_counts()","metadata":{"_uuid":"b8475696-143a-47eb-bd2c-20173d425e9d","_cell_guid":"180ed357-c11a-4073-ac5d-d91f227ea47d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.301119Z","iopub.status.idle":"2025-10-19T16:09:39.301429Z","shell.execute_reply.started":"2025-10-19T16:09:39.301253Z","shell.execute_reply":"2025-10-19T16:09:39.301267Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['Unit of Measurement'].value_counts()","metadata":{"_uuid":"2a5d9901-2523-4255-8c95-5df631cd94aa","_cell_guid":"5c05dbaf-ebfd-4025-942d-33c99868d580","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-10-19T16:09:39.303009Z","iopub.status.idle":"2025-10-19T16:09:39.303321Z","shell.execute_reply.started":"2025-10-19T16:09:39.303166Z","shell.execute_reply":"2025-10-19T16:09:39.303180Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}